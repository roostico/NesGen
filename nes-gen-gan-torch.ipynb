{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pretty_midi\n!pip install gdown\n!pip install miditok\n!pip install midi-clip\n\n!sudo apt install -y fluidsynth\n!pip install --upgrade pyfluidsynth\n\n!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/utility.py","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport glob\n\nimport os\nimport random\nimport shutil\nfrom pathlib import Path\nimport pretty_midi\nimport numpy as np\nfrom miditok import REMI, TokenizerConfig\nimport json\nfrom miditok.utils import split_files_for_training\nfrom miditok.data_augmentation import augment_dataset\nfrom random import shuffle\nfrom tqdm import tqdm\n\nimport sys\nimport pickle\n     ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get Maestro Dataset\n!wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n!unzip 'maestro-v3.0.0-midi.zip'\n!rm 'maestro-v3.0.0-midi.zip'\ndataset_path = \"/kaggle/working/maestro-v3.0.0\"","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to the files of the dataset\n\nmidi_paths = list(Path(dataset_path).resolve().glob(\"**/*.mid\")) + list(Path(dataset_path).resolve().glob(\"**/*.midi\"))\n\nmidis_dir = \"midis\"\nos.makedirs(midis_dir, exist_ok=True)\n\n\nfor i, midi_path in enumerate(midi_paths):\n  new_midi_path = os.path.join(midis_dir, f\"{i}.midi\")\n  shutil.move(str(midi_path), new_midi_path)\n\n\nmidis = list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.mid\")) + list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.midi\"))\n\ndef sample():\n  return str(random.choice(midis))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BEAT_RES = {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n\nTOKENIZER_PARAMS = {\n    \"pitch_range\": (21, 109),\n    \"beat_res\": BEAT_RES,\n    \"num_velocities\": 6,\n    \"special_tokens\": [\"BOS\", \"EOS\"],\n    \"use_chords\": True,\n    \"use_rests\": True,\n    \"use_tempos\": True,\n    \"num_tempos\": 8,\n    \"tempo_range\": (50, 200),  # (min_tempo, max_tempo),\n}\n\nconfig = TokenizerConfig(**TOKENIZER_PARAMS)\n\ntokenizer = REMI(config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = 1000\ntokenizer.train(vocab_size=vocab_size, files_paths=midis)\nprocessed = [Path(f\"{s}\") for s in midis]\nprint(len(processed))\n\nvalid_perc = 0.05\naugment = False\n\ntotal_num_files = len(processed)\nnum_files_valid = round(total_num_files * valid_perc)\nshuffle(processed)\nmidi_paths_valid = processed[:num_files_valid]\nmidi_paths_train = processed[num_files_valid:]\n\n# Chunk MIDIs and perform data augmentation on each subset independently\n\nfor files_paths, subset_name in (\n    (midi_paths_train, \"train\"),\n    (midi_paths_valid, \"valid\"),\n):\n    print(files_paths[0])\n\n    # Split the MIDIs into chunks of sizes approximately about 1024 tokens\n\n    subset_chunks_dir = Path(f\"Maestro_{subset_name}\")\n\n    split_files_for_training(\n        files_paths=files_paths,\n        tokenizer=tokenizer,\n        save_dir=subset_chunks_dir,\n        max_seq_len=1024,\n        num_overlap_bars=2,\n    )\n\n    # Perform data augmentation\n    if augment:\n        augment_dataset(\n            subset_chunks_dir,\n            pitch_offsets=[-12, 12],\n            velocity_offsets=[-3, 3],\n            duration_offsets=[-0.5, 0.5],\n        )\n\nmidi_paths_train = list(Path(\"Maestro_train\").glob(\"**/*.mid\")) + list(Path(\"Maestro_train\").glob(\"**/*.midi\"))\nmidi_paths_valid = list(Path(\"Maestro_valid\").glob(\"**/*.mid\")) + list(Path(\"Maestro_valid\").glob(\"**/*.midi\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def midi_valid(midi) -> bool:\n\n    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n\n        return False  # time signature different from 4/*, 4 beats per bar\n\n    return True\n\n\n\nif os.path.exists(\"tokenized\"):\n\n  shutil.rmtree(\"tokenized\")\n\n\nfor dir in (\"train\", \"valid\"):\n    tokenizer.tokenize_dataset(        \n    \n        Path(f\"/kaggle/working/Maestro_{dir}\"),\n        Path(f\"/kaggle/working/tokenized_{dir}\"),\n        midi_valid,\n    \n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_json(path: str) -> dict:\n\n  with open(path, \"r\") as f:\n\n    return json.load(f)\n\ndef read_json_files(json_file_paths):\n    \"\"\"Reads a list of JSON files and returns a list of objects.\n    Args:\n        json_file_paths: A list of file paths to JSON files.\n    Returns:\n        A list of objects, where each object represents the data from a JSON file.\n        Returns an empty list if any error occurs during file processing.\n    \"\"\"\n\n    objects = []\n\n    for file_path in tqdm(json_file_paths):\n\n        try:\n\n            objects.append(read_json(file_path))\n\n        except FileNotFoundError:\n\n            print(f\"Error: File not found - {file_path}\")\n\n            return [] # Return empty list on error\n\n        except json.JSONDecodeError:\n\n            print(f\"Error decoding JSON in file: {file_path}\")\n\n            return [] # Return empty list on error\n\n    return objects","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_train = list(Path(\"tokenized_train\").resolve().glob(\"**/*.json\"))\ndata_objects_train = read_json_files(tokenized_train)\n\ntokenized_valid = list(Path(\"tokenized_valid\").resolve().glob(\"**/*.json\"))\ndata_objects_valid = read_json_files(tokenized_valid)\n\nif data_objects_train:\n    print(f\"\\nSuccessfully read {len(data_objects_train)} training JSON files.\")\nelse:\n    print(\"Error reading JSON files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_train = [np.array(song[\"ids\"][0]) for song in data_objects_train]\nencoded_valid = [np.array(song[\"ids\"][0]) for song in data_objects_valid]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tokenizer.decode([encoded_train[0][:1024]]).dump_midi(\"sample.mid\")\nall_ids_train = np.concatenate(encoded_train)\nall_ids_valid = np.concatenate(encoded_valid)\nimport datetime\ntoday = datetime.datetime.today()\nday = today.day\nmonth = today.month\nname = \"tokenizer{:d}_{:02d}{:02d}.json\".format(vocab_size, month, day)\ntokenizer.save(name)\nnp.savetxt(\"ids_train_{:02d}{:02d}.txt\".format(month, day), all_ids_train)\nnp.savetxt(\"ids_valid_{:02d}{:02d}.txt\".format(month, day), all_ids_valid)\nall_ids_train = all_ids_train.astype(dtype=np.int32)\nall_ids_valid = all_ids_valid.astype(dtype=np.int32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if you need to skip all\n!gdown 1FqWFCW5TjWTI8rrbBy5uwYjS3GYl9MWF # tokenizer1000_1219.json\n!gdown 1Xs-5FenAaUJE_WipUDIFfQ4lFrK8VHve # ids_train_1219.txt\ntokenizer = REMI(params=\"tokenizer1000_1219.json\")\nall_ids_train = np.loadtxt(\"ids_train_1219.txt\").astype(dtype=np.int32)\n#all_ids_valid = np.loadtxt(\"ids_valid_1217.txt\").astype(dtype=np.int32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = len(tokenizer)\nseq_length = 512\nnormalized_seq = (all_ids_train - vocab_size / 2) / (vocab_size / 2)\n\n# Suddivisione in sequenze\nall_ids_train_seq = [normalized_seq[i:i + seq_length] \n                 for i in range(0, len(normalized_seq) - seq_length, seq_length)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LSTMGenerator(nn.Module):\n    def __init__(self, noise_dim, hidden_dim, seq_length, num_layers):\n        super(LSTMGenerator, self).__init__()\n        self.seq_length = seq_length\n        self.hidden_dim = hidden_dim\n        self.noise_dim = noise_dim\n        \n        # The linear layer should map the noise to (batch_size, hidden_dim * seq_length)\n        self.fc = nn.Linear(noise_dim, hidden_dim * seq_length)  # Map noise to the size that works with LSTM\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n        self.output_fc = nn.Linear(hidden_dim, 1)  # Output layer for each time step\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = x.squeeze(-1)  # Remove the last dimension -> (batch_size, noise_dim)\n        x = self.fc(x)  # Map noise to (batch_size, hidden_dim * seq_length)\n        x = x.view(-1, self.seq_length, self.hidden_dim)  # Reshape to (batch_size, seq_length, hidden_dim)\n        out, _ = self.lstm(x)  # Pass through LSTM\n        out = self.output_fc(out)  # Map hidden states to output dimension\n        return self.tanh(out)  # Shape: (batch_size, seq_length, 1)\n\nclass LSTMDiscriminator(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers):\n        super(LSTMDiscriminator, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)  # Predict a single scalar for real/fake\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out, _ = self.lstm(x)  # Pass through LSTM\n        out = out[:, -1, :]  # Use the last time step's hidden state: (batch_size, hidden_dim)\n        out = self.fc(out)  # Map to a single value: (batch_size, 1) \n        return out  # Real/fake probability\n\nclass GAN:\n    def __init__(self, \n                 generator_builder, \n                 discriminator_builder, \n                 noise_dim,\n                 seq_length,\n                 generator_layers,\n                 discriminator_layers,\n                 generator_hidden_dim,\n                 discriminator_hidden_dim,\n                 loss = nn.BCEWithLogitsLoss(),\n                ):\n        self.G = generator_builder(\n            noise_dim  = noise_dim,\n            hidden_dim = generator_hidden_dim,\n            seq_length = seq_length,\n            num_layers = generator_layers\n        )\n        self.D = discriminator_builder(\n            input_dim  = 1,\n            hidden_dim = discriminator_hidden_dim,\n            num_layers = discriminator_layers,\n        )\n        self.noise_dim = noise_dim\n        self.seq_length = seq_length\n        self.loss = loss\n        self.optimizer_G = optim.Adam(self.G.parameters(), lr=0.0002)\n        self.optimizer_D = optim.Adam(self.D.parameters(), lr=0.0002)\n\n    def train(\n        self, \n        dataloader, \n        epochs, \n        device, \n        warmup_epochs=0, \n        g_steps=0, \n        steps_each_print=5\n    ):\n        print(f\"Starting training with {epochs} epochs\")\n        self.G = self.G.to(device)\n        self.D = self.D.to(device)\n        \n        iteration_count = len(dataloader)  # Number of batches per epoch\n        batch_size = dataloader.batch_size\n        \n        for epoch in range(epochs):\n            pbar = tqdm(total=iteration_count, position=0, leave=True)\n            for step, real_data in enumerate(dataloader):\n                batch_size = real_data.size(0)\n                real_data = real_data.to(device).unsqueeze(-1)  # Shape: (batch_size, seq_length, 1)\n                \n                # Labels\n                real_labels = torch.ones(batch_size, 1).to(device)\n                fake_labels = torch.zeros(batch_size, 1).to(device)\n        \n                if epoch < warmup_epochs:  # Train Generator more during initial epochs\n                    for _ in range(g_steps):\n                        self.G.zero_grad()\n                        z = torch.randn(batch_size, noise_dim, 1).to(device)\n                        fake_data = self.G(z)\n                        outputs = self.D(fake_data)\n                        loss_G = self.loss(outputs, real_labels)\n                        loss_G.backward()\n                        self.optimizer_G.step()\n                \n                # Train Discriminator\n                self.D.zero_grad()\n                z = torch.randn(batch_size, self.noise_dim).to(device)  # Correct noise shape\n                fake_data = self.G(z)\n        \n                outputs_real = self.D(real_data)  # Pass real_data through Discriminator\n                loss_real = self.loss(outputs_real, real_labels)\n        \n                outputs_fake = self.D(fake_data)  # Pass fake_data through Discriminator\n                loss_fake = self.loss(outputs_fake, fake_labels)\n        \n                loss_D = loss_real + loss_fake\n                loss_D.backward()\n                self.optimizer_D.step()\n        \n                # Train Generator\n                self.G.zero_grad()\n                z = torch.randn(batch_size, noise_dim).to(device)\n                fake_data = self.G(z)\n        \n                outputs = self.D(fake_data)\n                loss_G = self.loss(outputs, real_labels)  # Generator wants to fool the Discriminator\n                loss_G.backward()\n                self.optimizer_G.step()\n                if (step + 1) % steps_each_print == 0:\n                    pbar.set_description(\n                        f\"D Loss: {loss_D.item():.4f}, \" +\n                        f\"G Loss: {loss_G.item():.4f}\"\n                    )\n                    pbar.update(steps_each_print)\n            pbar.n = pbar.total  \n            pbar.refresh()    \n            pbar.close()\n        \n            print(f'Epoch [{epoch+1}/{epochs}] Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}')\n\n    def generate(self, n, tokenizer, base_output_name, device):\n        z = torch.randn(n, self.noise_dim, 1)  # Rumore casuale\n        z = z.to(device)\n        generated_data = self.G(z).cpu().detach().numpy()\n\n        boundary = int(len(tokenizer) / 2)\n        predictions = [x * boundary + boundary for x in generated_data]\n        for i in range(len(predictions)):\n            pred = predictions[i]                                # [[5.0], [6.0], [7.0]]\n            pred_tokens = np.concatenate(pred).astype(np.int32)  # [5, 6, 7]\n            \n            decoded = tokenizer.decode([pred_tokens])\n            decoded.dump_midi(f\"{base_output_name}_{i}.mid\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parametri\ngenerator_dim = 256\ndiscriminator_dim = generator_dim // 2\nnum_layers = 2\nbatch_size = 128\nnoise_dim = 256\n\ngan = GAN(\n    LSTMGenerator,\n    LSTMDiscriminator,\n    noise_dim                = noise_dim,\n    seq_length               = seq_length,\n    generator_layers         = num_layers,\n    discriminator_layers     = num_layers,\n    generator_hidden_dim     = generator_dim,\n    discriminator_hidden_dim = discriminator_dim,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Dataset personalizzato\nclass TokenDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx], dtype=torch.float32)\n\ndataset = TokenDataset(all_ids_train_seq)\ndataloader = DataLoader(dataset, batch_size=batch_size)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training loop\nepochs = 1\nwarmup_epochs = 3\ng_steps = 0\nsteps_each_print = 5\n\ngan.train(\n    dataloader       = dataloader, \n    epochs           = epochs, \n    device           = device, \n    warmup_epochs    = warmup_epochs, \n    g_steps          = g_steps, \n    steps_each_print = steps_each_print\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_generations = 5\nbase_name = \"generated\"\n\ngan.generate(\n    n                 = n_generations,\n    tokenizer         = tokenizer, \n    base_output_name  = base_name, \n    device            = device,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython import display\n\n_SAMPLING_RATE = 16000\n\ndef display_audio(file, seconds=10):\n  pm = pretty_midi.PrettyMIDI(file)\n  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n  # Take a sample of the generated waveform to mitigate kernel resets\n  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n  return display.Audio(waveform_short, rate=_SAMPLING_RATE)\n\ndisplay_audio(f\"{base_name}_0.mid\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'generated.mid')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}