{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qzulj1okkrSq"
      },
      "outputs": [],
      "source": [
        "#@title Get a smaller version of the Lakh MIDI Dataset v0.1\n",
        "%%capture\n",
        "!wget http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz\n",
        "!tar xvf clean_midi.tar.gz\n",
        "!rm clean_midi.tar.gz\n",
        "!rm -rf sample_data\n",
        "\n",
        "dataset_path = \"/content/clean_midi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-E_cQreQk-8u"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pretty_midi\n",
        "!pip install miditok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.17\n",
        "!pip install --upgrade keras\n",
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "jUdRPfyzQEzj",
        "outputId": "afe8109d-3f7e-4420-a066-3d54af419ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "Successfully installed keras-3.6.0\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.17.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting keras-hub==0.17.0 (from keras_nlp)\n",
            "  Downloading keras_hub-0.17.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (2024.9.11)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (13.9.4)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras_nlp) (0.3.4)\n",
            "Collecting tensorflow-text (from keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras_nlp) (4.66.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras_nlp) (4.12.2)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text->keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (1.67.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.6.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-hub==0.17.0->keras_nlp) (2024.8.30)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.45.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.13.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras_nlp) (3.0.2)\n",
            "Downloading keras_nlp-0.17.0-py3-none-any.whl (2.0 kB)\n",
            "Downloading keras_hub-0.17.0-py3-none-any.whl (644 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow-text, keras-hub, keras_nlp\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-hub-0.17.0 keras_nlp-0.17.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing libraries to hear a MIDI\n",
        "%%capture\n",
        "!apt-get update -qq && apt-get install -y fluidsynth\n",
        "!pip install pretty_midi midi-clip\n",
        "\n",
        "# GS2\n",
        "!gdown 1wlpTIS70nQHMrYBjDT0M6nyg07kUejUv\n",
        "!unzip GeneralUser_GS_v2.0.0--doc_r2.zip\n",
        "!rm -rf GeneralUser_GS_v2.0.0--doc_r2.zip support documentation demo\\ MIDIs\n",
        "!mv GeneralUser\\ GS\\ v2.0.0.sf2 guGS.sf2\n",
        "\n",
        "# PICONICA\n",
        "!gdown 1uk51T9Gvo1n2JRl3_CHCg2FVGWiNI4qJ\n",
        "\n",
        "# Utility library\n",
        "!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/utility.py\n",
        "!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/transformer.py\n",
        "\n",
        "from utility import *"
      ],
      "metadata": {
        "id": "NHKsN90VswB-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "Ln0a94DmCede"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "from miditok import REMI, TokenizerConfig\n",
        "from utility import playMidi, show_midi_info\n",
        "import json\n",
        "import keras_nlp.layers as nlp_layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nmXk2h-HCf42"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skip = True"
      ],
      "metadata": {
        "id": "6rRwujaE-IUc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions"
      ],
      "metadata": {
        "id": "rvcrINLqacaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_tokens(tokens: np.ndarray, tokenizer: REMI, delete_after: bool = True, show_info: bool = False):\n",
        "  \"\"\"\n",
        "  Plays the given tokens, decoded using the given tokenizer\n",
        "  \"\"\"\n",
        "  dumped_midi = \"decoded.mid\"\n",
        "  tokenizer.decode([tokens]).dump_midi(dumped_midi)\n",
        "  to_play = playMidi(dumped_midi)\n",
        "  if show_info:\n",
        "    show_midi_info(dumped_midi)\n",
        "  if delete_after:\n",
        "    os.remove(dumped_midi)\n",
        "  return to_play\n",
        "\n",
        "\n",
        "def random_filtered(collection: list, predicate):\n",
        "  \"\"\"\n",
        "  Returns a random element from a collection that satisfies the given predicate.\n",
        "  If no element satisfies the filter, returns None.\n",
        "  \"\"\"\n",
        "  for elem in random.sample(collection, len(collection)):\n",
        "    if predicate(elem):\n",
        "      return elem\n",
        "\n",
        "def lasts_less_than(midi_path: str, time_seconds: int) -> bool:\n",
        "  \"\"\"\n",
        "  Returns true if the last note of the MIDI file is less than the given time in seconds.\n",
        "  \"\"\"\n",
        "  return pretty_midi.PrettyMIDI(midi_path).get_end_time() <= time_seconds"
      ],
      "metadata": {
        "id": "ExvDUjp7aeXD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move files and rename them"
      ],
      "metadata": {
        "id": "AEgwypV4k_-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a6elj9pecSnQ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Paths to the files of the dataset\n",
        "midi_paths = list(Path(dataset_path).resolve().glob(\"**/*.mid\"))\n",
        "\n",
        "midis_dir = \"midis\"\n",
        "os.makedirs(midis_dir, exist_ok=True)\n",
        "\n",
        "for i, midi_path in enumerate(midi_paths):\n",
        "  new_midi_path = os.path.join(midis_dir, f\"{i}.mid\")\n",
        "  shutil.move(str(midi_path), new_midi_path)\n",
        "\n",
        "midis = list(Path(\"midis\").resolve().glob(\"**/*.mid\"))\n",
        "\n",
        "def sample():\n",
        "  return str(random.choice(midis))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select a sample of these files"
      ],
      "metadata": {
        "id": "h2Y-PIyzlHmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a filtering function"
      ],
      "metadata": {
        "id": "Mas7DxYj991I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid(file: str) -> bool:\n",
        "  \"\"\"Checks if a MIDI file is valid. If any of its instruments has no name,\n",
        "  it is invalid.\n",
        "\n",
        "  Args:\n",
        "      file: The path to the MIDI file.\n",
        "\n",
        "  Returns:\n",
        "      True if the MIDI file is valid, False otherwise.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    midi = pretty_midi.PrettyMIDI(file)\n",
        "    if any([len(instrument.name) == 0 for instrument in midi.instruments]):\n",
        "      return False\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return False"
      ],
      "metadata": {
        "id": "sSMgrOb49FPA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move files"
      ],
      "metadata": {
        "id": "9vJmyA4g-CkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LxkoLRvYcmXj",
        "outputId": "8e2218c1-3ee0-4303-a2fe-4be405dc1cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 8/500 [00:01<01:30,  5.46it/s]/usr/local/lib/python3.10/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n",
            " 26%|██▌       | 129/500 [00:30<01:42,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no MTrk header at start of track\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███▏      | 157/500 [00:37<01:53,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIDI file has a largest tick of 33639950, it is likely corrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 184/500 [00:52<01:16,  4.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 204/500 [01:01<02:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 234/500 [01:07<00:58,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▉     | 247/500 [01:10<00:45,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 356/500 [01:37<00:20,  6.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running status without last_status\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▍  | 373/500 [01:40<00:29,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 401/500 [01:49<00:24,  4.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not decode key with 2 flats and mode 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 408/500 [01:51<00:25,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [02:13<00:00,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data byte must be in range 0..127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def copy_random_files(source_dir: str, dest_dir: str, num_files: int, is_file_valid) -> list:\n",
        "  \"\"\"Copies a specified number of random files from a source directory to a destination directory.\n",
        "\n",
        "  Args:\n",
        "      source_dir: The path to the source directory.\n",
        "      dest_dir: The path to the destination directory.\n",
        "      num_files: The number of files to move.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(source_dir):\n",
        "    print(f\"Error: Source directory '{source_dir}' not found.\")\n",
        "    return\n",
        "\n",
        "  if os.path.exists(dest_dir):\n",
        "    shutil.rmtree(dest_dir)\n",
        "\n",
        "  os.makedirs(dest_dir, exist_ok=True)\n",
        "  files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "  if len(files) < num_files:\n",
        "    print(f\"Warning: Only {len(files)} files found in '{source_dir}'. Moving all of them.\")\n",
        "    num_files = len(files)\n",
        "\n",
        "  files_to_move = []\n",
        "  i = 0\n",
        "  with tqdm(total=num_files, position=0, leave=True) as pbar:\n",
        "    while i < num_files:\n",
        "      random_file = random.choice(files)\n",
        "      if is_file_valid(os.path.join(source_dir, random_file)):\n",
        "        files_to_move.append(random_file)\n",
        "        i = i + 1\n",
        "        pbar.update()\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  result = []\n",
        "  for file in files_to_move:\n",
        "    source_path = os.path.join(source_dir, file)\n",
        "    dest_path = os.path.join(dest_dir, file)\n",
        "    shutil.copy(source_path, dest_path)\n",
        "    result.append(dest_path)\n",
        "  return result\n",
        "\n",
        "source_directory = \"midis\"\n",
        "destination_directory = \"selected\"\n",
        "number_of_files_to_move = 500\n",
        "\n",
        "sample = copy_random_files(source_directory, destination_directory, number_of_files_to_move, is_valid)\n",
        "assert len(sample) == 500"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listen one random MIDI from sample"
      ],
      "metadata": {
        "id": "fmvUMw52BdxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not skip:\n",
        "  def valid_example(file: str) -> float:\n",
        "    \"\"\"\n",
        "    Returns true if the midi file lasts less that 120 seconds and has more than 1 instrument\n",
        "    \"\"\"\n",
        "    return lasts_less_than(file, 120) and len(pretty_midi.PrettyMIDI(file).instruments) > 1\n",
        "\n",
        "  example = random_filtered(sample, valid_example)\n",
        "\n",
        "  print(f\"Showing file {example}\")\n",
        "  show_midi_info(example)\n",
        "  playMidi(example)"
      ],
      "metadata": {
        "id": "B7sfptC6Bhan"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "RA677BGpNMNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRACK_MIN_DENSITY_PERC = 0.20\n",
        "\n",
        "def is_excluded(track_name):\n",
        "  \"\"\"\n",
        "  Exclusion criteria for MIDI tracks\n",
        "  \"\"\"\n",
        "  exclude_keywords = [\"drum\", \"effect\", \"percussion\"]\n",
        "  return any(keyword.lower() in track_name.lower() for keyword in exclude_keywords)\n",
        "\n",
        "def merge_tracks_to_single_instrument(input_file, output_file, target_channel=0):\n",
        "    midi_data = pretty_midi.PrettyMIDI(input_file)\n",
        "    merged_midi = pretty_midi.PrettyMIDI()\n",
        "    merged_instrument = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program('Acoustic Grand Piano'), is_drum=False)\n",
        "\n",
        "    mean_notes = map(lambda x: len(x.notes), midi_data.instruments)\n",
        "    mean_notes = sum(mean_notes) / len(midi_data.instruments)\n",
        "\n",
        "    for instrument in midi_data.instruments:\n",
        "        track_name = instrument.name\n",
        "\n",
        "        # Exclude drum instruments or effect instruments\n",
        "        if instrument.is_drum or is_excluded(track_name):\n",
        "            continue\n",
        "\n",
        "        # Exclude instruments that have a low number of notes\n",
        "        if len(instrument.notes) / mean_notes < TRACK_MIN_DENSITY_PERC:\n",
        "            continue\n",
        "\n",
        "        for note in instrument.notes:\n",
        "            note.velocity = max(1, note.velocity)  # Ensure velocity is within MIDI range\n",
        "            merged_instrument.notes.append(note)\n",
        "\n",
        "    merged_instrument.notes.sort(key=lambda note: note.start)\n",
        "\n",
        "    tempo_times, tempi = midi_data.get_tempo_changes()\n",
        "    if len(tempi) > 0:\n",
        "        merged_midi._tick_scales = midi_data._tick_scales  # Copy tempo-related timing\n",
        "\n",
        "    merged_midi.instruments.append(merged_instrument)\n",
        "    merged_midi.time_signature_changes = midi_data.time_signature_changes\n",
        "    merged_midi.key_signature_changes = midi_data.key_signature_changes\n",
        "\n",
        "    merged_midi.write(output_file)\n",
        "\n",
        "if os.path.exists(\"pre-processed\"):\n",
        "  shutil.rmtree(\"pre-processed\")\n",
        "\n",
        "os.makedirs(\"pre-processed\")\n",
        "\n",
        "for file in tqdm(sample):\n",
        "  try:\n",
        "    merge_tracks_to_single_instrument(file, f\"pre-processed/{os.path.basename(file)}\")\n",
        "  except Exception as e:\n",
        "    print(f\"There was an error: {e}\")\n",
        "    continue\n",
        "\n",
        "processed = list(Path(\"pre-processed\").resolve().glob(\"**/*.mid\"))"
      ],
      "metadata": {
        "id": "8bRjAI8HNOA5",
        "outputId": "92f1d76b-62a5-4e40-a17a-3b3069d9978f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listen the same file from before, but with merged MIDI tracks"
      ],
      "metadata": {
        "id": "bqUCP3U-HEa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not skip:\n",
        "  processed_example = os.path.join(\"pre-processed\", os.path.basename(example))\n",
        "  print(f\"Showing file {processed_example}\")\n",
        "  show_midi_info(processed_example)\n",
        "  playMidi(processed_example)"
      ],
      "metadata": {
        "id": "_zFSNP-zHQOX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "X_qYrXTErrsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed = list(Path(\"pre-processed\").resolve().glob(\"**/*.mid\"))"
      ],
      "metadata": {
        "id": "2dF6SlcbQaPQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_config = {\n",
        "    \"use_pitchdrum_tokens\": False\n",
        "}\n",
        "\n",
        "tok_config = TokenizerConfig(**tok_config)\n",
        "tokenizer = REMI(tok_config)"
      ],
      "metadata": {
        "id": "fM44SXMEHNmo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional): train the tokenizer"
      ],
      "metadata": {
        "id": "VqhBEeOGHUZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(vocab_size=1000, files_paths=processed)"
      ],
      "metadata": {
        "id": "ELI4Z2GhHZFB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer the dataset"
      ],
      "metadata": {
        "id": "CGr1Z52HHaaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def midi_valid(midi) -> bool:\n",
        "    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n",
        "        return False  # time signature different from 4/*, 4 beats per bar\n",
        "    return True\n",
        "\n",
        "if os.path.exists(\"tokenized\"):\n",
        "  shutil.rmtree(\"tokenized\")\n",
        "\n",
        "tokenizer.tokenize_dataset(        # 2 velocity and 1 duration values\n",
        "    Path(\"/content\", \"pre-processed\"),\n",
        "    Path(\"/content\", \"tokenized\"),\n",
        "    midi_valid,\n",
        ")"
      ],
      "metadata": {
        "id": "uB8MAhdvHGEB",
        "outputId": "abe3f495-a771-4778-ef51-2ce450d5efd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing music files (content/tokenized): 100%|██████████| 493/493 [00:40<00:00, 12.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function to read a JSON tokenized file"
      ],
      "metadata": {
        "id": "_txg7WlqOoXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(path: str) -> dict:\n",
        "  with open(path, \"r\") as f:\n",
        "    return json.load(f)"
      ],
      "metadata": {
        "id": "bZzHxEPeOrnU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the tokenized version of the previous file"
      ],
      "metadata": {
        "id": "K8sNToqWOcs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not skip:\n",
        "  tokenized_example = os.path.join(\"tokenized\", Path(example).stem + \".json\")\n",
        "  example_ids = read_json(tokenized_example)[\"ids\"][0]\n",
        "  print(f\"Showing IDS of {tokenized_example}\")\n",
        "  print(np.array(example_ids))"
      ],
      "metadata": {
        "id": "ZVpb2XnBOgSC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the tokenized version of files from the JSON"
      ],
      "metadata": {
        "id": "ceLrlILzQg7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_files(json_file_paths):\n",
        "    \"\"\"Reads a list of JSON files and returns a list of objects.\n",
        "\n",
        "    Args:\n",
        "        json_file_paths: A list of file paths to JSON files.\n",
        "\n",
        "    Returns:\n",
        "        A list of objects, where each object represents the data from a JSON file.\n",
        "        Returns an empty list if any error occurs during file processing.\n",
        "    \"\"\"\n",
        "    objects = []\n",
        "    for file_path in tqdm(json_file_paths):\n",
        "        try:\n",
        "            objects.append(read_json(file_path))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found - {file_path}\")\n",
        "            return [] # Return empty list on error\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error decoding JSON in file: {file_path}\")\n",
        "            return [] # Return empty list on error\n",
        "    return objects\n",
        "\n",
        "# Example usage (assuming 'tokenized' directory contains JSON files):\n",
        "tokenized_files = list(Path(\"/content\", \"tokenized\").resolve().glob(\"**/*.json\"))\n",
        "data_objects = read_json_files(tokenized_files)\n",
        "\n",
        "if data_objects:\n",
        "    print(f\"\\nSuccessfully read {len(data_objects)} JSON files.\")\n",
        "    # Now you can work with the 'data_objects' list\n",
        "    # For example, print the first object:\n",
        "    # print(data_objects[0])\n",
        "else:\n",
        "    print(\"Error reading JSON files.\")"
      ],
      "metadata": {
        "id": "iR0bPrk-H1O1",
        "outputId": "a2ad76f1-ff8e-4a77-903d-a159a8723800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 493/493 [00:00<00:00, 768.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully read 493 JSON files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the list of tokenized songs, taking the IDs of each one"
      ],
      "metadata": {
        "id": "P9nbkZgfQllX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = [np.array(song[\"ids\"][0]) for song in data_objects]"
      ],
      "metadata": {
        "id": "_B81lpcTKL6m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listen the same example from before, decoded from its tokenization"
      ],
      "metadata": {
        "id": "Cg9YYbdlSpzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not skip:\n",
        "  print(f\"Showing decoded IDS of {tokenized_example}\")\n",
        "\n",
        "  to_play = play_tokens(example_ids, tokenizer, show_info = True)\n",
        "  to_play"
      ],
      "metadata": {
        "id": "EZU2w5GtSxF0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trim of initial and ending silence in each song"
      ],
      "metadata": {
        "id": "BsY64PWJQ7Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim(ids: np.ndarray, token_to_remove: int) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Returns a new numpy array with initial and ending `token_to_remove` removed.\n",
        "  \"\"\"\n",
        "  start_idx = 0\n",
        "  end_idx = len(ids)\n",
        "\n",
        "  while start_idx < len(ids) and ids[start_idx] == token_to_remove:\n",
        "      start_idx += 1\n",
        "  while end_idx > start_idx and ids[end_idx - 1] == token_to_remove:\n",
        "      end_idx -= 1\n",
        "  return ids[start_idx:end_idx]\n",
        "\n",
        "bar_token = tokenizer.vocab[\"Bar_None\"]\n",
        "encoded = [trim(ids, bar_token) for ids in encoded]"
      ],
      "metadata": {
        "id": "wNAB9oeoRBwE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove long rests in the song"
      ],
      "metadata": {
        "id": "E_KK0LtlZoMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shorten_sequences(arr: np.ndarray, target_value: int, max_length: int) -> np.ndarray:\n",
        "  result = []\n",
        "  current_sequence = []\n",
        "\n",
        "  for value in arr:\n",
        "    if value == target_value:\n",
        "      current_sequence.append(value)\n",
        "    else:\n",
        "      if len(current_sequence) > max_length:\n",
        "        result.extend([target_value] * max_length)\n",
        "      else:\n",
        "        result.extend(current_sequence)\n",
        "\n",
        "      current_sequence = []\n",
        "      result.append(value)\n",
        "\n",
        "  if len(current_sequence) > max_length:\n",
        "    result.extend([target_value] * max_length)\n",
        "  else:\n",
        "    result.extend(current_sequence)\n",
        "\n",
        "  return np.array(result)\n",
        "\n",
        "bar_token = tokenizer.vocab[\"Bar_None\"]\n",
        "max_rest_length = 5\n",
        "encoded = [shorten_sequences(ids, bar_token, max_rest_length) for ids in encoded]"
      ],
      "metadata": {
        "id": "D_iZoc74Zr_Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show an example from the resulting IDs"
      ],
      "metadata": {
        "id": "W2YwfFIKZdg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not skip:\n",
        "  random_ids = random.choice(encoded)[:1000]\n",
        "\n",
        "  print(\"Decoded:\")\n",
        "  play_tokens(random_ids, tokenizer, show_info = True)"
      ],
      "metadata": {
        "id": "0PTftO6OZidw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End of pre-processing, proceding with data and model preparation with Tensorflow\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VmJjxVpaQr13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow data and model setup"
      ],
      "metadata": {
        "id": "MFobYqPQQ2mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Tensorflow dataset with all IDs"
      ],
      "metadata": {
        "id": "PZYhXakurnaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "all_ids = np.concatenate(encoded)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "0yIWzFmmowXC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert into sequences"
      ],
      "metadata": {
        "id": "gKA8lKB5srKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 400 #@param {type: 'slider', max: 500, min: 50, step: 50}\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "X33uqO3KrWNA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing labels"
      ],
      "metadata": {
        "id": "ZnJUygt7uUFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_seq = sequence[:-1]\n",
        "    target_seq = sequence[1:]\n",
        "    return input_seq, target_seq\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Xlsk46y3sVRS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating training batches"
      ],
      "metadata": {
        "id": "o6Ssr4f_um6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 32 #@param {type: 'slider', max: 256, min: 32, step: 32}\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "G1nUYxujuorC",
        "outputId": "a5d2411c-75dc-4328-c4ce-a7dd98512bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(32, 400), dtype=tf.float64, name=None), TensorSpec(shape=(32, 400), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting in Train, Validation and Test"
      ],
      "metadata": {
        "id": "sryXDHvk6ElG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_size = dataset.cardinality().numpy()\n",
        "train_size = int(0.8 * ds_size)\n",
        "val_size = int(0.1 * ds_size)\n",
        "test_size = int(0.1 * ds_size)"
      ],
      "metadata": {
        "id": "pRUgrcIV6HPI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(train_size)\n",
        "remaining = dataset.skip(train_size)\n",
        "valid_ds = remaining.take(val_size)\n",
        "test_ds = remaining.skip(val_size)"
      ],
      "metadata": {
        "id": "a6u4odFU6dhT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "13TNnkIRuwKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "Hj1_6V3FCVXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras NLP"
      ],
      "metadata": {
        "id": "XVkiWMzFL1vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(seq_length,\n",
        "                 vocab_size,\n",
        "                 model_dim=256,\n",
        "                 norm_epsilon=1e-5,\n",
        "                 dropout=0.1,\n",
        "                 num_layers=3,\n",
        "                 intermediate_dim=512,\n",
        "                 num_heads=4\n",
        "                 ):\n",
        "  inputs = keras.Input(shape=(seq_length,))\n",
        "\n",
        "  # Embed our tokens with a positional embedding.\n",
        "  embedding_layer = nlp_layers.TokenAndPositionEmbedding(\n",
        "      vocabulary_size=vocab_size,\n",
        "      sequence_length=seq_length,\n",
        "      embedding_dim=model_dim,\n",
        "  )\n",
        "  outputs = embedding_layer(inputs)\n",
        "\n",
        "  # Apply layer normalization and dropout to the embedding.\n",
        "  outputs = keras.layers.LayerNormalization(epsilon=norm_epsilon)(outputs)\n",
        "  outputs = keras.layers.Dropout(rate=dropout)(outputs)\n",
        "\n",
        "  # Add a number of encoder blocks\n",
        "  for i in range(num_layers):\n",
        "      outputs = nlp_layers.TransformerEncoder(\n",
        "          intermediate_dim=intermediate_dim,\n",
        "          num_heads=num_heads,\n",
        "          dropout=dropout,\n",
        "          layer_norm_epsilon=norm_epsilon,\n",
        "      )(outputs)\n",
        "\n",
        "  outputs = keras.layers.Dense(units=vocab_size)(outputs)\n",
        "\n",
        "  return keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "tPKGtwx0L3MG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(seq_length, tokenizer.vocab_size)\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "LEARNING_RATE = 5e-4\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer=keras.optimizers.AdamW(LEARNING_RATE),\n",
        "              weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
        "              jit_compile=True,\n",
        "              )"
      ],
      "metadata": {
        "id": "Y7yE9MuLMyrp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "vIfM0aScNI0p"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=valid_ds,\n",
        "                    callbacks=[checkpoint_callback, early_stopping]\n",
        "                    )"
      ],
      "metadata": {
        "id": "Tpzre8kENI0p",
        "outputId": "7bb0c20d-da42-46d3-f24c-749b806f59a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-37-c0dcf454099a>\", line 3, in <cell line: 3>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_11533]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c0dcf454099a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_ds,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-37-c0dcf454099a>\", line 3, in <cell line: 3>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_iterator_11533]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "Q2cEIaBKg55B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_step(model, seed):\n",
        "  predictions = model(seed)\n",
        "  sampled_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
        "  sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "  return sampled_indices\n",
        "\n",
        "for seed_ids, _ in test_ds.take(1):\n",
        "  seed = seed_ids\n",
        "seed = seed[0]"
      ],
      "metadata": {
        "id": "zqVJaweJg7Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seed)"
      ],
      "metadata": {
        "id": "jSaBXYpl6RSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "midi = tokenizer.decode([seed])\n",
        "midi.dump_midi(\"nesgen.mid\")\n",
        "playMidi(\"nesgen.mid\")"
      ],
      "metadata": {
        "id": "l1inB38P5_Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(seed, model, length = 5):\n",
        "  import time\n",
        "  start = time.time()\n",
        "  next_ids = seed\n",
        "  result = [next_ids]\n",
        "  n = 0\n",
        "  for n in tqdm(range(length)):\n",
        "    next_ids = generate_one_step(model, tf.expand_dims(next_ids, axis=0))\n",
        "    result.append(next_ids)\n",
        "\n",
        "  end = time.time()\n",
        "  print('\\nRun time:', end - start, \"\\n\", '_'*80, \"\\n\")\n",
        "  result = np.concatenate(result[1:])\n",
        "  print(\"Shape of result: \", result.shape)\n",
        "  return result"
      ],
      "metadata": {
        "id": "juiaXSKXihat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate(seed, model, 5)"
      ],
      "metadata": {
        "id": "pPEptMWFjNme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "midi = tokenizer.decode([result])\n",
        "midi.dump_midi(\"nesgen.mid\")\n",
        "playMidi(\"nesgen.mid\")"
      ],
      "metadata": {
        "id": "NTc8PNiwkJKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD"
      ],
      "metadata": {
        "id": "Jl1LT5wONJbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters"
      ],
      "metadata": {
        "id": "Wr6h6007A7K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 256\n",
        "num_heads = 2\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "INYXNRtkA5zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer import Transformer\n",
        "\n",
        "model = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizer.vocab_size,\n",
        "    target_vocab_size=tokenizer.vocab_size,\n",
        "    dropout_rate=dropout_rate\n",
        "    )"
      ],
      "metadata": {
        "id": "EKXVOp2v7EyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UJlS7YtFBM0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YW9ZJXh4Buwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try generating with un-trained model"
      ],
      "metadata": {
        "id": "Cv7uUEpv3LUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utility import playMidi\n",
        "\n",
        "# Use the model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# Sample indices from predictions\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "# Decode\n",
        "midi = tokenizer.decode([sampled_indices])\n",
        "midi.dump_midi(\"boh5.mid\")\n",
        "playMidi(\"boh5.mid\")"
      ],
      "metadata": {
        "id": "2L95vK8M0grD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss\n",
        "#loss = tf.keras.losses.sparse_categorical_crossentropy(target_example_batch, example_batch_predictions, from_logits=True)\n",
        "# Reduce mean to get a single scalar loss value\n",
        "#loss = tf.reduce_mean(loss)\n",
        "\n",
        "#print(\"Loss:\", loss.numpy())"
      ],
      "metadata": {
        "id": "Bgq7J1sTy-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taken from tensorflow tutorial:\n",
        "\n",
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ],
      "metadata": {
        "id": "od6NsuqV8y2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Checking if it is near to vocabulary size\")\n",
        "#print(tf.exp(example_batch_mean_loss).numpy())\n",
        "#print(\"Vocab size: \", encoding.vocab_size)"
      ],
      "metadata": {
        "id": "MXnmou978tLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "CdAbc4nvy_Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "e9hwzFK6zDYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "lADTb-9GzVsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(train_ds,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=valid_ds,\n",
        "                    callbacks=[checkpoint_callback, early_stopping]\n",
        "                    )"
      ],
      "metadata": {
        "id": "4EWiw3RYzo_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "wQp_3lVX-wLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, decoding, encoding, vocab_size, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.decode = decoding\n",
        "    self.encode = encoding\n",
        "\n",
        "    # Taken from tensorflow tutorial: useful to skip ids\n",
        "\n",
        "    #skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    #sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "    #    values=[-float('inf')]*len(skip_ids),\n",
        "    #    indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "    #    dense_shape=[vocab_size])\n",
        "    #self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, input_ids):\n",
        "    input_ids_ = tf.expand_dims(input_ids, axis=0)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits = self.model(inputs=input_ids_)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Taken from tensorflow tutorial: apply prediction mask to prevent certain\n",
        "    # ids from being generated\n",
        "    #predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the ids and model state.\n",
        "\n",
        "    return predicted_ids"
      ],
      "metadata": {
        "id": "_2z0FLUK-yYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, tokenizer.decode, tokenizer.encode, tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "bgXr-8m9AWVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "midis = list(Path(\"/content/midis\").glob(\"*.mid\"))"
      ],
      "metadata": {
        "id": "cD3URdtFU8iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_file() -> str:\n",
        "  return random.choice(midis)\n",
        "\n",
        "seed_ids = np.array(tokenizer.encode(get_random_file())[0].ids)\n",
        "seed_ids = seed_ids[:seq_length]\n",
        "print(seed_ids)"
      ],
      "metadata": {
        "id": "yN7j8IoUCQxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "one_step_model.generate_one_step(seed_ids)"
      ],
      "metadata": {
        "id": "lg1mzRUWAMDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "next_ids = seed_ids\n",
        "result = [next_ids]\n",
        "n = 0\n",
        "while n < 1000:\n",
        "  next_ids = one_step_model.generate_one_step(next_ids)\n",
        "  if next_ids == tokenizer.vocab[\"Bar_None\"]:\n",
        "    continue\n",
        "  n = n + 1\n",
        "  result.append(next_ids)\n",
        "\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start, \"\\n\", '_'*80, \"\\n\")\n",
        "result = np.concatenate(result[1:])\n",
        "print(\"Shape of result: \", result.shape)"
      ],
      "metadata": {
        "id": "ND5aYRs-Agfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the generator"
      ],
      "metadata": {
        "id": "hC3N64vLT8gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')"
      ],
      "metadata": {
        "id": "89POLl6ETDLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload the generator"
      ],
      "metadata": {
        "id": "PBcd9qc1VmiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "MFxLMn6TVolF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_from_ids(ids, tokenizer):\n",
        "  tokens = []\n",
        "  for id in ids:\n",
        "    for key, value in tokenizer.vocab.items():\n",
        "      if value == id:\n",
        "        tokens.append(key)\n",
        "  return np.array(tokens)"
      ],
      "metadata": {
        "id": "sgnRbYJ228eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = train_ds.take(1).as_numpy_iterator().next()[0]"
      ],
      "metadata": {
        "id": "oGMKVG8H40xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_from_ids(example[0], tokenizer)"
      ],
      "metadata": {
        "id": "wWETYlrn4sl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_from_ids(result, tokenizer)"
      ],
      "metadata": {
        "id": "URAWsmhN5Zbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hear the result"
      ],
      "metadata": {
        "id": "6BuVmTnKT_xl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIjhe6oqa12m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_from_ids(result, tokenizer))"
      ],
      "metadata": {
        "id": "3zb2B27MbYfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utility import playMidi\n",
        "\n",
        "midi = tokenizer.decode([result])\n",
        "midi.dump_midi(\"result.mid\")\n",
        "playMidi(\"result.mid\")"
      ],
      "metadata": {
        "id": "w8tRExb_T-f4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}