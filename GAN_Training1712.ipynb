{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!pip install pretty_midi\n!pip install miditok\n!pip install midi-clip\n\n!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/utility.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:11:59.144284Z","iopub.execute_input":"2024-12-17T11:11:59.144625Z","iopub.status.idle":"2024-12-17T11:12:38.863447Z","shell.execute_reply.started":"2024-12-17T11:11:59.144594Z","shell.execute_reply":"2024-12-17T11:12:38.862555Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport pretty_midi\nimport numpy as np\nfrom miditok import REMI, TokenizerConfig\nimport json\nimport tensorflow as tf\nfrom miditok.utils import split_files_for_training\nfrom miditok.data_augmentation import augment_dataset\nimport random\nfrom random import shuffle\n\nimport sys\nimport pickle\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:12:38.865226Z","iopub.execute_input":"2024-12-17T11:12:38.865520Z","iopub.status.idle":"2024-12-17T11:12:53.717062Z","shell.execute_reply.started":"2024-12-17T11:12:38.865493Z","shell.execute_reply":"2024-12-17T11:12:53.716218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data section","metadata":{}},{"cell_type":"code","source":"!wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n!unzip \"maestro-v3.0.0-midi.zip\"\n!rm \"maestro-v3.0.0-midi.zip\"\n\ndataset_path = \"/kaggle/working/maestro-v3.0.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:41:30.240764Z","iopub.execute_input":"2024-12-17T09:41:30.241249Z","iopub.status.idle":"2024-12-17T09:41:35.329162Z","shell.execute_reply.started":"2024-12-17T09:41:30.241218Z","shell.execute_reply":"2024-12-17T09:41:35.328053Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare the dataset","metadata":{}},{"cell_type":"code","source":"# Paths to the files of the dataset\n\nmidi_paths = list(Path(dataset_path).resolve().glob(\"**/*.mid\")) + list(Path(dataset_path).resolve().glob(\"**/*.midi\"))\n\nmidis_dir = \"midis\"\nos.makedirs(midis_dir, exist_ok=True)\n\n\nfor i, midi_path in enumerate(midi_paths):\n  new_midi_path = os.path.join(midis_dir, f\"{i}.midi\")\n  shutil.move(str(midi_path), new_midi_path)\n\n\nmidis = list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.mid\")) + list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.midi\"))\n\ndef sample():\n  return str(random.choice(midis))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:41:35.333020Z","iopub.execute_input":"2024-12-17T09:41:35.333550Z","iopub.status.idle":"2024-12-17T09:41:35.396526Z","shell.execute_reply.started":"2024-12-17T09:41:35.333520Z","shell.execute_reply":"2024-12-17T09:41:35.395882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BEAT_RES = {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n\nTOKENIZER_PARAMS = {\n    \"pitch_range\": (21, 109),\n    \"beat_res\": BEAT_RES,\n    \"num_velocities\": 6,\n    \"special_tokens\": [\"BOS\", \"EOS\"],\n    \"use_chords\": True,\n    \"use_rests\": True,\n    \"use_tempos\": True,\n    \"num_tempos\": 8,\n    \"tempo_range\": (50, 200),  # (min_tempo, max_tempo),\n}\n\nconfig = TokenizerConfig(**TOKENIZER_PARAMS)\n\ntokenizer = REMI(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:41:35.397457Z","iopub.execute_input":"2024-12-17T09:41:35.397706Z","iopub.status.idle":"2024-12-17T09:41:35.405397Z","shell.execute_reply.started":"2024-12-17T09:41:35.397682Z","shell.execute_reply":"2024-12-17T09:41:35.404569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = 1000\ntokenizer.train(vocab_size=vocab_size, files_paths=midis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:41:35.430249Z","iopub.execute_input":"2024-12-17T09:41:35.430557Z","iopub.status.idle":"2024-12-17T09:47:08.024997Z","shell.execute_reply.started":"2024-12-17T09:41:35.430531Z","shell.execute_reply":"2024-12-17T09:47:08.024052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed = [Path(f\"{s}\") for s in midis]\nprint(len(processed))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:41:35.406452Z","iopub.execute_input":"2024-12-17T09:41:35.406716Z","iopub.status.idle":"2024-12-17T09:41:35.429220Z","shell.execute_reply.started":"2024-12-17T09:41:35.406694Z","shell.execute_reply":"2024-12-17T09:41:35.428401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_perc = 0.05\naugment = False\n\ntotal_num_files = len(processed)\nnum_files_valid = round(total_num_files * valid_perc)\nshuffle(processed)\nmidi_paths_valid = processed[:num_files_valid]\nmidi_paths_train = processed[num_files_valid:]\n\n# Chunk MIDIs and perform data augmentation on each subset independently\n\nfor files_paths, subset_name in (\n    (midi_paths_train, \"train\"),\n    (midi_paths_valid, \"valid\"),\n):\n    print(files_paths[0])\n\n    # Split the MIDIs into chunks of sizes approximately about 1024 tokens\n\n    subset_chunks_dir = Path(f\"Maestro_{subset_name}\")\n\n    split_files_for_training(\n        files_paths=files_paths,\n        tokenizer=tokenizer,\n        save_dir=subset_chunks_dir,\n        max_seq_len=1024,\n        num_overlap_bars=2,\n    )\n\n    # Perform data augmentation\n    if augment:\n        augment_dataset(\n            subset_chunks_dir,\n            pitch_offsets=[-12, 12],\n            velocity_offsets=[-3, 3],\n            duration_offsets=[-0.5, 0.5],\n        )\n\nmidi_paths_train = list(Path(\"Maestro_train\").glob(\"**/*.mid\")) + list(Path(\"Maestro_train\").glob(\"**/*.midi\"))\nmidi_paths_valid = list(Path(\"Maestro_valid\").glob(\"**/*.mid\")) + list(Path(\"Maestro_valid\").glob(\"**/*.midi\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:47:08.027010Z","iopub.execute_input":"2024-12-17T09:47:08.027282Z","iopub.status.idle":"2024-12-17T09:48:21.214294Z","shell.execute_reply.started":"2024-12-17T09:47:08.027257Z","shell.execute_reply":"2024-12-17T09:48:21.213614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def midi_valid(midi) -> bool:\n\n    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n\n        return False  # time signature different from 4/*, 4 beats per bar\n\n    return True\n\n\n\nif os.path.exists(\"tokenized\"):\n\n  shutil.rmtree(\"tokenized\")\n\n\nfor dir in (\"train\", \"valid\"):\n    tokenizer.tokenize_dataset(        \n    \n        Path(f\"/kaggle/working/Maestro_{dir}\"),\n        Path(f\"/kaggle/working/tokenized_{dir}\"),\n        midi_valid,\n    \n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:48:21.215285Z","iopub.execute_input":"2024-12-17T09:48:21.215593Z","iopub.status.idle":"2024-12-17T09:54:00.910051Z","shell.execute_reply.started":"2024-12-17T09:48:21.215561Z","shell.execute_reply":"2024-12-17T09:54:00.909207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_json(path: str) -> dict:\n\n  with open(path, \"r\") as f:\n\n    return json.load(f)\n\ndef read_json_files(json_file_paths):\n    \"\"\"Reads a list of JSON files and returns a list of objects.\n    Args:\n        json_file_paths: A list of file paths to JSON files.\n    Returns:\n        A list of objects, where each object represents the data from a JSON file.\n        Returns an empty list if any error occurs during file processing.\n    \"\"\"\n\n    objects = []\n\n    for file_path in tqdm(json_file_paths):\n\n        try:\n\n            objects.append(read_json(file_path))\n\n        except FileNotFoundError:\n\n            print(f\"Error: File not found - {file_path}\")\n\n            return [] # Return empty list on error\n\n        except json.JSONDecodeError:\n\n            print(f\"Error decoding JSON in file: {file_path}\")\n\n            return [] # Return empty list on error\n\n    return objects\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:00.911518Z","iopub.execute_input":"2024-12-17T09:54:00.911895Z","iopub.status.idle":"2024-12-17T09:54:00.919494Z","shell.execute_reply.started":"2024-12-17T09:54:00.911858Z","shell.execute_reply":"2024-12-17T09:54:00.918465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_train = list(Path(\"tokenized_train\").resolve().glob(\"**/*.json\"))\ndata_objects_train = read_json_files(tokenized_train)\n\ntokenized_valid = list(Path(\"tokenized_valid\").resolve().glob(\"**/*.json\"))\ndata_objects_valid = read_json_files(tokenized_valid)\n\nif data_objects_train:\n    print(f\"\\nSuccessfully read {len(data_objects_train)} training JSON files.\")\nelse:\n    print(\"Error reading JSON files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:00.920545Z","iopub.execute_input":"2024-12-17T09:54:00.920905Z","iopub.status.idle":"2024-12-17T09:54:03.510259Z","shell.execute_reply.started":"2024-12-17T09:54:00.920863Z","shell.execute_reply":"2024-12-17T09:54:03.509393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_train = [np.array(song[\"ids\"][0]) for song in data_objects_train]\nencoded_valid = [np.array(song[\"ids\"][0]) for song in data_objects_valid]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:03.511200Z","iopub.execute_input":"2024-12-17T09:54:03.511500Z","iopub.status.idle":"2024-12-17T09:54:04.268455Z","shell.execute_reply.started":"2024-12-17T09:54:03.511474Z","shell.execute_reply":"2024-12-17T09:54:04.267687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### (Optional) decode one piece","metadata":{}},{"cell_type":"code","source":"tokenizer.decode([encoded_train[0][:1024]]).dump_midi(\"sample.mid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:30:42.846058Z","iopub.execute_input":"2024-12-16T16:30:42.846421Z","iopub.status.idle":"2024-12-16T16:30:42.855574Z","shell.execute_reply.started":"2024-12-16T16:30:42.846390Z","shell.execute_reply":"2024-12-16T16:30:42.854733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_ids_train = np.concatenate(encoded_train)\nall_ids_valid = np.concatenate(encoded_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:04.269419Z","iopub.execute_input":"2024-12-17T09:54:04.269672Z","iopub.status.idle":"2024-12-17T09:54:04.321963Z","shell.execute_reply.started":"2024-12-17T09:54:04.269649Z","shell.execute_reply":"2024-12-17T09:54:04.321224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datetime\ntoday = datetime.datetime.today()\nday = today.day\nmonth = today.month\nname = \"tokenizer{:d}_{:02d}{:02d}.json\".format(vocab_size, month, day)\ntokenizer.save(name)\nnp.savetxt(\"ids_train_{:02d}{:02d}.txt\".format(month, day), all_ids_train)\nnp.savetxt(\"ids_valid_{:02d}{:02d}.txt\".format(month, day), all_ids_valid)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:04.322978Z","iopub.execute_input":"2024-12-17T09:54:04.323215Z","iopub.status.idle":"2024-12-17T09:54:28.896493Z","shell.execute_reply.started":"2024-12-17T09:54:04.323192Z","shell.execute_reply":"2024-12-17T09:54:28.895424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_ids_train = all_ids_train.astype(dtype=np.int32)\nall_ids_valid = all_ids_valid.astype(dtype=np.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T09:54:28.897932Z","iopub.execute_input":"2024-12-17T09:54:28.898288Z","iopub.status.idle":"2024-12-17T09:54:28.929106Z","shell.execute_reply.started":"2024-12-17T09:54:28.898252Z","shell.execute_reply":"2024-12-17T09:54:28.928396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ... or skip all the data preparation","metadata":{}},{"cell_type":"code","source":"!gdown 1ZIPjenm4tEzAKPc-ONE4gYLzILR3YYqe # tokenizer1000_1217.json\n!gdown 1LN8wrTcUOzlPkQs7Gh-RD9Z2ftbua_E6 # ids_train_1217.txt\n!gdown 12SOuWNUM9ofo5dhGWvNEj09c_dYisB7g # ids_valid_1217.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:12:53.718318Z","iopub.execute_input":"2024-12-17T11:12:53.719190Z","iopub.status.idle":"2024-12-17T11:13:27.552578Z","shell.execute_reply.started":"2024-12-17T11:12:53.719126Z","shell.execute_reply":"2024-12-17T11:13:27.551402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = REMI(params=\"tokenizer1000_1217.json\")\nall_ids_train = np.loadtxt(\"ids_train_1217.txt\").astype(dtype=np.int32)\nall_ids_valid = np.loadtxt(\"ids_valid_1217.txt\").astype(dtype=np.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:27.555929Z","iopub.execute_input":"2024-12-17T11:13:27.556392Z","iopub.status.idle":"2024-12-17T11:13:30.914824Z","shell.execute_reply.started":"2024-12-17T11:13:27.556362Z","shell.execute_reply":"2024-12-17T11:13:30.914049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tensorflow datasets","metadata":{}},{"cell_type":"markdown","source":"### Recommended: limit arrays","metadata":{}},{"cell_type":"code","source":"perc = 1\nall_ids_train = all_ids_train[:int(perc * len(all_ids_train))]\nall_ids_valid = all_ids_valid[:int(perc * len(all_ids_valid))]\nprint(f\"Loaded {len(all_ids_train)} training ids\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:30.915851Z","iopub.execute_input":"2024-12-17T11:13:30.916111Z","iopub.status.idle":"2024-12-17T11:13:30.921213Z","shell.execute_reply.started":"2024-12-17T11:13:30.916085Z","shell.execute_reply":"2024-12-17T11:13:30.920369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids_dataset_train = tf.data.Dataset.from_tensor_slices(all_ids_train)\nids_dataset_valid = tf.data.Dataset.from_tensor_slices(all_ids_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:30.922311Z","iopub.execute_input":"2024-12-17T11:13:30.922525Z","iopub.status.idle":"2024-12-17T11:13:31.617971Z","shell.execute_reply.started":"2024-12-17T11:13:30.922503Z","shell.execute_reply":"2024-12-17T11:13:31.617286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq_length = 512\nvocab_size = len(tokenizer)\nBATCH_SIZE = 128\nBUFFER_SIZE = 60000\n\ndef normalize(sequence):\n    # Convert to float32\n    input_seq = tf.cast(sequence, tf.float32)\n    normalized_seq = (input_seq - vocab_size / 2) / (vocab_size / 2)\n    return normalized_seq\n\ntrain_ds = (\n    ids_dataset_train\n    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n    .map(normalize)\n    .map(lambda x: (tf.expand_dims(x, -1)))  # Add channel dimension: (seq_length, 1)\n    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n    .shuffle(BUFFER_SIZE)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\nvalid_ds = (\n    ids_dataset_valid\n    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n    .map(normalize)\n    .map(lambda x: (tf.expand_dims(x, -1)))  # Add channel dimension: (seq_length, 1)\n    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n    .shuffle(BUFFER_SIZE)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\nfor real_seqs in train_ds.take(1):\n    print(f\"Input Shape: {real_seqs.shape}, Input Type: {real_seqs.dtype}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:31.618938Z","iopub.execute_input":"2024-12-17T11:13:31.619212Z","iopub.status.idle":"2024-12-17T11:13:48.548660Z","shell.execute_reply.started":"2024-12-17T11:13:31.619185Z","shell.execute_reply":"2024-12-17T11:13:48.547721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import GRU\n\ndef generator(latent_dim, seq_shape): \n    model = Sequential()\n    model.add(Input(shape=(latent_dim, 1)))\n    model.add(GRU(512, input_shape=(latent_dim, 1), return_sequences=True))\n    model.add(Bidirectional(GRU(512)))\n    model.add(Dense(256))\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(512))\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(1024))\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n    model.add(Reshape(seq_shape))\n    return model\n\ndef discriminator(seq_shape):\n    model = Sequential()\n    model.add(Input(shape=seq_shape))\n    model.add(GRU(512, input_shape=seq_shape, return_sequences=True))\n    model.add(Bidirectional(GRU(512, return_sequences=True)))\n    model.add(Dense(512))\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(Dense(512))\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(Dense(1))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:48.549881Z","iopub.execute_input":"2024-12-17T11:13:48.550273Z","iopub.status.idle":"2024-12-17T11:13:48.558376Z","shell.execute_reply.started":"2024-12-17T11:13:48.550240Z","shell.execute_reply":"2024-12-17T11:13:48.557380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Smaller version","metadata":{}},{"cell_type":"code","source":"class GAN():\n  def __init__(self, vocab_size, seq_length, latent_dim = 512):\n    self.vocab_size = vocab_size\n    self.seq_length = seq_length\n    self.seq_shape = (self.seq_length, 1)\n    self.latent_dim = latent_dim\n    self.disc_loss = []\n    self.gen_loss = []\n\n    self.loss_fun = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    self.generator_opt = tf.keras.optimizers.Adam(1e-4)\n    self.discriminator_opt = tf.keras.optimizers.Adam(1e-4)\n    self.disc_accuracy = tf.keras.metrics.BinaryAccuracy() \n\n    # Build and compile the discriminator\n    self.discriminator = discriminator(self.seq_shape)\n      \n    # Build the generator\n    self.generator = generator(self.latent_dim, self.seq_shape)\n\n    # The generator takes noise as input and generates note sequences\n    z = Input(shape=(self.latent_dim, 1))\n    generated_seq = self.generator(z)\n\n    validity = self.discriminator(generated_seq)\n\n    # The combined model  (stacked generator and discriminator)\n    # Trains the generator to fool the discriminator\n    self.combined = Model(z, validity)\n      \n  def _discriminator_loss(self, real_output, fake_output):\n    real_loss = self.loss_fun(tf.ones_like(real_output), real_output)\n    fake_loss = self.loss_fun(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n      \n  def _generator_loss(self, fake_output):\n    return self.loss_fun(tf.ones_like(fake_output), fake_output)\n\n  @tf.function\n  def _train_step(self, real_batch_x, batch_size):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # GENERATOR -> FAKE BATCH\n        noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n        fake_batch_x = self.generator(noise, training=True)  # Shape: (batch_size, seq_length, 1)\n\n        real_output = self.discriminator(real_batch_x, training=True)\n        fake_output = self.discriminator(fake_batch_x, training=True)\n\n        gen_loss = self._generator_loss(fake_output)\n        disc_loss = self._discriminator_loss(real_output, fake_output)\n\n        disc_labels = tf.concat((tf.ones_like(real_output), tf.zeros_like(fake_output)), axis=0)\n        disc_output = tf.concat((real_output, fake_output), axis=0)\n        self.disc_accuracy.update_state(disc_labels, disc_output)\n    \n    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n\n    self.generator_opt.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n    self.discriminator_opt.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n    return (gen_loss, disc_loss)\n      \n  def train(self, epochs, batch_size, train_dataset, valid_dataset, steps_each_print=50):\n    print(\"\\nStarting Training\\n\")\n    iteration_count = len(train_dataset)\n\n    for epoch in range(epochs):\n        print(\"\\nStart of epoch %d\" % (epoch + 1,))\n        pbar = tqdm(total=iteration_count, position=0, leave=True)\n        \n        for step, real_seqs in enumerate(train_dataset):\n            gen_loss, disc_loss = self._train_step(real_seqs, batch_size)\n            \n            if step % steps_each_print == 0:\n                pbar.set_description(\n                    f\"D Loss: {disc_loss:.4f}, \" +\n                    f\"D Accuracy: {100 * self.disc_accuracy.result():.2f}%, \" +\n                    f\"G Loss: {gen_loss:.4f}\"\n                )\n                pbar.update(steps_each_print)\n        pbar.n = pbar.total  \n        pbar.refresh()    \n        pbar.close()\n        print(f\"Epoch {epoch} complete. Discriminator accuracy: {self.disc_accuracy.result()}\")\n    \n    print(\"\\nTraining Complete.\\n\")\n        \n  def save(self):\n    # create Model directory if there isn't exist\n    if not os.path.exists('Model/'):\n      os.makedirs('Model/')\n\n    # save discriminator and generator trained model\n    self.discriminator.save('Model/discriminator.h5')\n    self.generator.save('Model/generator.h5')\n    print(\"The trained C-RNN-GAN model (generator and discriminator) have been saved in the Model folder.\")\n\n\n  def generate(self):\n    \"\"\" Use random noise to generate music\"\"\"\n    \n    # random noise for network input\n    noise = np.random.normal(0, 1, (BATCH_SIZE, self.latent_dim, 1))\n    predictions = self.generator.predict(noise)\n\n    # transfer sequence numbers to notes\n    boundary = int(self.vocab_size / 2)\n    pred_nums = [x * boundary + boundary for x in predictions[0]]\n    return pred_nums\n\n\n  def plot_loss(self):\n    \"\"\" Plot and save discriminator and generator loss functions per epoch diagram\"\"\"\n    plt.plot(self.disc_loss, c='red')\n    plt.plot(self.gen_loss, c='blue')\n    plt.title(\"GAN Loss per Epoch\")\n    plt.legend(['Discriminator', 'Generator'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n    plt.savefig('Result/GAN_Loss_per_Epoch_final.png', transparent=True)\n    plt.close()\n\nmodel = GAN(vocab_size, seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:48.559765Z","iopub.execute_input":"2024-12-17T11:13:48.560115Z","iopub.status.idle":"2024-12-17T11:13:49.906048Z","shell.execute_reply.started":"2024-12-17T11:13:48.560077Z","shell.execute_reply":"2024-12-17T11:13:49.905083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hide prints","metadata":{}},{"cell_type":"code","source":"tf.get_logger().setLevel('ERROR')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T16:40:56.837353Z","iopub.execute_input":"2024-12-16T16:40:56.837707Z","iopub.status.idle":"2024-12-16T16:40:56.842412Z","shell.execute_reply.started":"2024-12-16T16:40:56.837676Z","shell.execute_reply":"2024-12-16T16:40:56.841475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 50\n\n\nmodel.train(EPOCHS, BATCH_SIZE, train_ds, valid_ds, steps_each_print=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:13:49.908299Z","iopub.execute_input":"2024-12-17T11:13:49.908588Z","iopub.status.idle":"2024-12-17T16:29:32.479307Z","shell.execute_reply.started":"2024-12-17T11:13:49.908559Z","shell.execute_reply":"2024-12-17T16:29:32.478335Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:31:36.175115Z","iopub.execute_input":"2024-12-17T16:31:36.175507Z","iopub.status.idle":"2024-12-17T16:31:36.291100Z","shell.execute_reply.started":"2024-12-17T16:31:36.175473Z","shell.execute_reply":"2024-12-17T16:31:36.290226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r model.zip Model/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:31:36.436620Z","iopub.execute_input":"2024-12-17T16:31:36.436950Z","iopub.status.idle":"2024-12-17T16:31:39.543950Z","shell.execute_reply.started":"2024-12-17T16:31:36.436920Z","shell.execute_reply":"2024-12-17T16:31:39.542779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_ids = np.concatenate(model.generate()).astype(np.int32)\nprint(generated_ids)\nfor index, id in enumerate(generated_ids):\n    if id >= len(tokenizer):\n        print(f\"Found id {id}, setting to {len(tokenizer) - 1}\")\n        generated_ids[index] = len(tokenizer) - 1\ndecoded = tokenizer.decode([generated_ids])\ndecoded.dump_midi(\"generated.mid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:29:42.418798Z","iopub.execute_input":"2024-12-17T16:29:42.419857Z","iopub.status.idle":"2024-12-17T16:29:43.160710Z","shell.execute_reply.started":"2024-12-17T16:29:42.419817Z","shell.execute_reply":"2024-12-17T16:29:43.159966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}