{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roostico/NesGen/blob/main/NESGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4-w5fIGbm3-"
      },
      "source": [
        "# Getting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vO6_YgvYQIg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get the full version of the Lakh MIDI Dataset v0.1\n",
        "!wget http://hog.ee.columbia.edu/craffel/lmd/lmd_full.tar.gz\n",
        "!tar xvf lmd_full.tar.gz\n",
        "!rm lmd_full.tar.gz\n",
        "\n",
        "dataset_path = \"/content/lmd_full\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "khHJHOl-buMg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Get a smaller version of the Lakh MIDI Dataset v0.1\n",
        "!wget http://hog.ee.columbia.edu/craffel/lmd/clean_midi.tar.gz\n",
        "!tar xvf clean_midi.tar.gz\n",
        "!rm clean_midi.tar.gz\n",
        "\n",
        "dataset_path = \"/content/clean_midi\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get the NESMDB dataset\n",
        "!gdown 1gIli7G1wu0QWDLzRc-CPWB8C4Hu0XVn3\n",
        "!unzip nesmdb_midi.zip\n",
        "!rm nesmdb_midi.zip"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "bkOU_DPSFtzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "1fNB6qpNyd3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing MidiTok"
      ],
      "metadata": {
        "id": "EGpo-rT2yiwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miditok"
      ],
      "metadata": {
        "id": "DQK88OIcyfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the tokenizer"
      ],
      "metadata": {
        "id": "8jrmoX-xyoBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from miditok import REMI, TokenizerConfig  # here we choose to use REMI\n",
        "from pathlib import Path\n",
        "\n",
        "# Creates the tokenizer and list the file paths\n",
        "tokenizer = REMI()  # using defaults parameters (constants.py)\n"
      ],
      "metadata": {
        "id": "UxhOz5KUyqGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmenting and splitting the dataset"
      ],
      "metadata": {
        "id": "WzTTmzJ2ClEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "from miditok.data_augmentation import augment_dataset\n",
        "from miditok.utils import split_files_for_training\n",
        "\n",
        "# Split the dataset into train/valid/test subsets, with 15% of the data for each of the two latter\n",
        "midi_paths = list(Path(dataset_path).glob(\"**/*.mid\"))[:1000]\n",
        "total_num_files = len(midi_paths)\n",
        "num_files_valid = round(total_num_files * 0.15)\n",
        "num_files_test = round(total_num_files * 0.15)\n",
        "shuffle(midi_paths)\n",
        "midi_paths_valid = midi_paths[:num_files_valid]\n",
        "midi_paths_test = midi_paths[num_files_valid:num_files_valid + num_files_test]\n",
        "midi_paths_train = midi_paths[num_files_valid + num_files_test:]\n",
        "\n",
        "# Chunk MIDIs and perform data augmentation on each subset independently\n",
        "for files_paths, subset_name in (\n",
        "    (midi_paths_train, \"train\"), (midi_paths_valid, \"valid\"), (midi_paths_test, \"test\")\n",
        "):\n",
        "\n",
        "    # Split the MIDIs into chunks of sizes approximately about 1024 tokens\n",
        "    subset_chunks_dir = Path(f\"dataset_{subset_name}\")\n",
        "    split_files_for_training(\n",
        "        files_paths=files_paths,\n",
        "        tokenizer=tokenizer,\n",
        "        save_dir=subset_chunks_dir,\n",
        "        max_seq_len=1024,\n",
        "        num_overlap_bars=2,\n",
        "    )\n",
        "\n",
        "    # Perform data augmentation\n",
        "    augment_dataset(\n",
        "        subset_chunks_dir,\n",
        "        pitch_offsets=[-12, 12],\n",
        "        velocity_offsets=[-4, 4],\n",
        "        duration_offsets=[-0.5, 0.5],\n",
        "    )"
      ],
      "metadata": {
        "id": "kbPtY22J6Bif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data loading"
      ],
      "metadata": {
        "id": "H2CWTd-nCoTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "midi_paths = list(Path(\"dataset_train\").glob(\"**/*.mid\")) + list(Path(\"dataset_valid\").glob(\"**/*.mid\")) + list(Path(\"dataset_test\").glob(\"**/*.mid\"))\n",
        "\n",
        "# A validation method to discard MIDIs we do not want\n",
        "# It can also be used for custom pre-processing, for instance if you want to merge\n",
        "# some tracks before tokenizing a MIDI file\n",
        "def midi_valid(midi) -> bool:\n",
        "    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n",
        "        return False  # time signature different from 4/*, 4 beats per bar\n",
        "    return True\n",
        "\n",
        "# Builds the vocabulary with BPE\n",
        "# tokenizer.train(vocab_size=30000, files_paths=midi_paths)\n",
        "dataset = DatasetMIDI(\n",
        "    files_paths=midi_paths,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=128,\n",
        "    bos_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer[\"BOS_None\"],\n",
        ")\n",
        "\n",
        "collator = DataCollator(tokenizer.pad_token_id)\n",
        "data_loader = DataLoader(dataset=dataset, collate_fn=collator)"
      ],
      "metadata": {
        "id": "1rJwmDwhy0mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "6joF2HOjFwMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an example model"
      ],
      "metadata": {
        "id": "s998irZECdY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras_nlp import layers as nlp_layers\n",
        "\n",
        "def build_transformer_model(vocab_size, seq_length, num_heads, ff_dim):\n",
        "    # Input layers for token ids and attention mask\n",
        "    input_ids = layers.Input(shape=(seq_length,), dtype='int32', name='input_ids')\n",
        "    attention_mask = layers.Input(shape=(seq_length,), dtype='int32', name='attention_mask')\n",
        "\n",
        "    # Embedding layer for token ids\n",
        "    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=128)(input_ids)\n",
        "\n",
        "    # Transformer Encoder\n",
        "    transformer_layer = nlp_layers.TransformerEncoder(\n",
        "        num_heads=num_heads,\n",
        "        intermediate_dim=ff_dim,\n",
        "        dropout=0.1\n",
        "    )(embedding_layer)\n",
        "\n",
        "    # Output layer (logits for each token in the sequence)\n",
        "    output = layers.Dense(vocab_size, activation='softmax')(transformer_layer)\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 30522  # Based on your tokenizer\n",
        "seq_length = 128    # Maximum sequence length\n",
        "num_heads = 8       # Number of attention heads\n",
        "ff_dim = 512        # Feed-forward layer size\n",
        "\n",
        "model = build_transformer_model(vocab_size, seq_length, num_heads, ff_dim)"
      ],
      "metadata": {
        "id": "HJmCcKlIEnWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example training"
      ],
      "metadata": {
        "id": "ZdvDqU78Kgdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "i = 0\n",
        "# Assuming input_ids_np and labels_np are the input tokens and shifted labels\n",
        "for batch in tqdm(data_loader):\n",
        "    input_ids_np = batch['input_ids'].cpu().numpy()\n",
        "    attention_mask_np = batch['attention_mask'].cpu().numpy()\n",
        "    labels_np = np.roll(input_ids_np, shift=-1, axis=1)  # Shifted input_ids as labels\n",
        "    labels_np[:, -1] = 0  # Optionally mask the last token\n",
        "\n",
        "    # Train the Keras model on the batch\n",
        "    model.train_on_batch([input_ids_np, attention_mask_np], labels_np)\n",
        "    i = i + 1\n",
        "    if i == 50:\n",
        "      break"
      ],
      "metadata": {
        "id": "xoYqPaoKEruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility function to generate a new sequence of tokens"
      ],
      "metadata": {
        "id": "ZTdynTMnKjpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model, tokenizer, max_length, prompt, attention_mask, vocab_size):\n",
        "    \"\"\"\n",
        "    Generate a sequence using the Transformer model with greedy decoding.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Keras transformer model\n",
        "    - tokenizer: Tokenizer used to convert tokens to IDs and vice-versa\n",
        "    - max_length: Maximum length of the generated sequence\n",
        "    - prompt: Initial sequence of token IDs to start generating from\n",
        "    - attention_mask: Initial attention mask for the input\n",
        "    - vocab_size: The size of the vocabulary\n",
        "\n",
        "    Returns:\n",
        "    - Generated sequence of token IDs\n",
        "    \"\"\"\n",
        "    generated_sequence = prompt  # Start with the initial prompt\n",
        "    current_length = len(generated_sequence[0])  # Current length of the sequence\n",
        "\n",
        "    while current_length < max_length:\n",
        "        # Prepare inputs for the model (pad the sequence if necessary)\n",
        "        input_ids_np = np.array(generated_sequence)\n",
        "        attention_mask_np = np.array(attention_mask)\n",
        "\n",
        "        # Predict the next token (get logits from the model)\n",
        "        logits = model.predict([input_ids_np, attention_mask_np], verbose=0)\n",
        "\n",
        "        # Take the last timestep logits (logits for the next token in sequence)\n",
        "        next_token_logits = logits[:, current_length - 1, :]\n",
        "\n",
        "        # Use greedy decoding: pick the token with the highest probability\n",
        "        next_token_id = np.argmax(next_token_logits, axis=-1)\n",
        "\n",
        "        # Append the predicted token to the sequence\n",
        "        generated_sequence[0].append(next_token_id[0])\n",
        "\n",
        "        # Update attention mask (append 1 for the new token)\n",
        "        attention_mask[0].append(1)\n",
        "\n",
        "        current_length += 1\n",
        "\n",
        "        # Break if the end token is generated (assuming 0 is the end token)\n",
        "        #if next_token_id[0] == tokenizer.eos_token_id:\n",
        "        #    break\n",
        "\n",
        "    return generated_sequence[0]"
      ],
      "metadata": {
        "id": "cOd815ceHLok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "Dwjk3cMlKpG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [[4, 5, 100, 56, 49, 10]]  # Initial prompt in token IDs\n",
        "attention_mask = [[1] * len(prompt[0])]  # Initial attention mask (1s for the tokens in the prompt)\n",
        "\n",
        "max_length = 50  # Set the maximum length for the generated sequence\n",
        "vocab_size = 30522  # Set your vocabulary size\n",
        "\n",
        "# Generate a sequence\n",
        "generated_ids = generate_sequence(model, tokenizer, max_length, prompt, attention_mask, vocab_size)\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "#generated_text = tokenizer.decode(generated_ids)\n",
        "print(generated_ids)"
      ],
      "metadata": {
        "id": "HMyvv58lHSSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation of the model"
      ],
      "metadata": {
        "id": "pyHf8xZWCtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "Sx3D0NKrCzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of the model"
      ],
      "metadata": {
        "id": "aR83A2dqC0tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "FX_StJK6C2db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPKJ0WAsd1pT"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtbAeJt-d4ML"
      },
      "outputs": [],
      "source": [
        "def random_file(root, keyword=None):\n",
        "    import glob\n",
        "    import os\n",
        "    import random\n",
        "    mid_files = glob.glob(os.path.join(root, \"**\", \"*.mid\"), recursive=True)\n",
        "    if keyword is not None:\n",
        "      mid_files = [file for file in mid_files if keyword in file.lower()]\n",
        "    return random.choice(mid_files)\n",
        "\n",
        "def generate_midi_from_tokens(tokens, tokenizer, output_path):\n",
        "  from pathlib import Path\n",
        "  # Convert to MIDI and save it\n",
        "  generated_midi = tokenizer(tokens)  # MidiTok can handle PyTorch/Numpy/Tensorflow tensors\n",
        "  generated_midi.dump_midi(Path(output_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_midi_from_tokens([generated_ids], tokenizer, \"generated.mid\")"
      ],
      "metadata": {
        "id": "B50R7eJbIbvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N90HWz7Z8nc"
      },
      "source": [
        "# MIDI playing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WJCh_UeaBlI"
      },
      "source": [
        "## Installing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZEn9Xq4aAWO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -y fluidsynth\n",
        "!pip install pretty_midi midi-clip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download example Soundfonts (GeneralUser GS v2 and PICONICA)"
      ],
      "metadata": {
        "id": "iCSNhDFNkSxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1wlpTIS70nQHMrYBjDT0M6nyg07kUejUv\n",
        "!unzip GeneralUser_GS_v2.0.0--doc_r2.zip\n",
        "!rm -rf GeneralUser_GS_v2.0.0--doc_r2.zip support documentation demo\\ MIDIs\n",
        "!mv GeneralUser\\ GS\\ v2.0.0.sf2 guGS.sf2\n",
        "\n",
        "# PICONICA\n",
        "!gdown 1uk51T9Gvo1n2JRl3_CHCg2FVGWiNI4qJ"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vYLKV8QBgJi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: download other soundfonts"
      ],
      "metadata": {
        "id": "jGNPg_MgwsII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pokemon\n",
        "!gdown 1vDK_xH7WeAqQrrBFXfh4Q205x6oNhTQt"
      ],
      "metadata": {
        "id": "jButvEyvwx8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN6kSFkZaWce"
      },
      "source": [
        "## Utility function to generate the audio on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taken from https://github.com/bzamecnik/midi2audio/blob/master/midi2audio.py"
      ],
      "metadata": {
        "id": "GsvltbQjMO2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "__all__ = ['FluidSynth']\n",
        "\n",
        "DEFAULT_SOUND_FONT = '~/.fluidsynth/default_sound_font.sf2'\n",
        "DEFAULT_SAMPLE_RATE = 44100\n",
        "DEFAULT_GAIN = 0.2\n",
        "\n",
        "class FluidSynth():\n",
        "    def __init__(self, sound_font=DEFAULT_SOUND_FONT, sample_rate=DEFAULT_SAMPLE_RATE, gain=DEFAULT_GAIN):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.sound_font = os.path.expanduser(sound_font)\n",
        "        self.gain = gain\n",
        "\n",
        "    def midi_to_audio(self, midi_file: str, audio_file: str, verbose=True):\n",
        "        if verbose:\n",
        "            stdout = None\n",
        "        else:\n",
        "            stdout = subprocess.DEVNULL\n",
        "        subprocess.call(\n",
        "            ['fluidsynth', '-ni', '-g', str(self.gain), self.sound_font, midi_file, '-F', audio_file, '-r', str(self.sample_rate)],\n",
        "            stdout=stdout,\n",
        "        )\n",
        "\n",
        "    def play_midi(self, midi_file):\n",
        "        subprocess.call(['fluidsynth', '-i', '-g', str(self.gain), self.sound_font, midi_file, '-r', str(self.sample_rate)])"
      ],
      "metadata": {
        "id": "JAgDunjjMJTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other utility functions"
      ],
      "metadata": {
        "id": "XGGrotfQMk1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WINfP4LzacAg"
      },
      "outputs": [],
      "source": [
        "import pretty_midi\n",
        "import os\n",
        "import librosa.display\n",
        "\n",
        "def show_midi_info(midi_path, print_notes=False):\n",
        "  midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "  print(\"Instruments: \", [instrument.name for instrument in midi_data.instruments])\n",
        "  print(\"MIDI duration: {duration:.2f} seconds\".format(duration=midi_data.get_end_time()))\n",
        "  if print_notes:\n",
        "    for instrument in midi_data.instruments:\n",
        "      print(instrument.name)\n",
        "      for note in instrument.notes:\n",
        "        print(note.start, note.end, note.pitch, note.velocity)\n",
        "\n",
        "def piano_roll(midi_path):\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plot_piano_roll(path, 24, 84)\n",
        "\n",
        "def plot_piano_roll(path, start_pitch, end_pitch, fs=100):\n",
        "    midi_data = pretty_midi.PrettyMIDI(path)\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    librosa.display.specshow(midi_data.get_piano_roll(fs)[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "def change_midi_velocity(midi_path, output_path, delta=0): # Renamed the function to avoid name conflict\n",
        "  midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "  if delta != 0:\n",
        "    for instrument in midi_data.instruments:\n",
        "      for note in instrument.notes:\n",
        "        note.velocity += delta\n",
        "    midi_data.write(output_path)\n",
        "\n",
        "def convert_midi_to_wav(soundfont_path, midi_path, output_path, gain=None, velocity_change=0): # Renamed the argument\n",
        "  change_midi_velocity(midi_path, \"temp.mid\", delta=velocity_change) # Call the renamed function\n",
        "  FluidSynth(soundfont_path, gain=gain).midi_to_audio(\"temp.mid\", output_path)\n",
        "  os.remove(\"temp.mid\")\n",
        "\n",
        "\n",
        "def trim_midi(midi_path, start, end):\n",
        "  import mido\n",
        "  import midi_clip\n",
        "  mid = mido.MidiFile(midi_path)\n",
        "  trimmed_midi = midi_clip.midi_clip(mid, start, end)\n",
        "\n",
        "  dir_name, base_name = os.path.split(midi_path)\n",
        "  new_base_name = \"trimmed_\" + base_name\n",
        "  output_path = os.path.join(dir_name, new_base_name)\n",
        "  trimmed_midi.save(output_path)\n",
        "  return output_path\n",
        "\n",
        "def playMidi(midi_file_path,\n",
        "             soundfont_path=\"/content/guGS.sf2\",\n",
        "             output_path=\"audio.wav\",\n",
        "             start=None,\n",
        "             end=None,\n",
        "             gain=DEFAULT_GAIN,\n",
        "             velocity_change=0\n",
        "             ):\n",
        "    from IPython.display import Audio\n",
        "\n",
        "    if start is not None and end is not None:\n",
        "      midi_file_path = trim_midi(midi_file_path, start, end)\n",
        "      convert_midi_to_wav(soundfont_path, midi_file_path, output_path, gain=gain, velocity_change=velocity_change)\n",
        "      os.remove(midi_file_path)\n",
        "    else:\n",
        "      convert_midi_to_wav(soundfont_path, midi_file_path, output_path, gain=gain, velocity_change=velocity_change)\n",
        "    return Audio(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path = random_file(dataset_path)\n",
        "print(\"Converting: \" + path)\n",
        "print(\"Midi info:\")\n",
        "show_midi_info(path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gbk6J2o2aXo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HZVkX8iIY6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CithkaLAafKv"
      },
      "source": [
        "## Play a random MIDI of the Lakh dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUE9qdC2aiOs"
      },
      "outputs": [],
      "source": [
        "path = random_file(dataset_path)\n",
        "print(\"Converting: \" + path)\n",
        "print(\"Midi info:\")\n",
        "show_midi_info(path)\n",
        "print(\"Synthetized:\")\n",
        "playMidi(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play a random MIDI of the NESMDB dataset"
      ],
      "metadata": {
        "id": "RN-UiflHNYQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = random_file(\"nesmdb_midi\")\n",
        "print(\"Converting: \" + path)\n",
        "print(\"Midi info:\")\n",
        "show_midi_info(path)\n",
        "print(\"Synthetized:\")\n",
        "playMidi(path, soundfont_path=\"PICONICA.sf2\", velocity_change=30, gain=1)"
      ],
      "metadata": {
        "id": "K-1eJsT7NX_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_midi_info(\"generated.mid\")"
      ],
      "metadata": {
        "id": "mdjx9A_lJKDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "playMidi(\"generated.mid\", soundfont_path=\"PICONICA.sf2\")"
      ],
      "metadata": {
        "id": "bGxg43R8Iu3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}