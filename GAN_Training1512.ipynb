{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T14:42:36.455669Z",
     "iopub.status.busy": "2024-12-15T14:42:36.455309Z",
     "iopub.status.idle": "2024-12-15T14:43:16.462283Z",
     "shell.execute_reply": "2024-12-15T14:43:16.461441Z",
     "shell.execute_reply.started": "2024-12-15T14:42:36.455638Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install pretty_midi\n",
    "!pip install miditok\n",
    "!pip install midi-clip\n",
    "\n",
    "!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/utility.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.837586Z",
     "iopub.status.idle": "2024-12-13T15:58:01.837926Z",
     "shell.execute_reply": "2024-12-13T15:58:01.837788Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.837770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:43:31.094104Z",
     "iopub.status.busy": "2024-12-15T14:43:31.093491Z",
     "iopub.status.idle": "2024-12-15T14:43:31.099839Z",
     "shell.execute_reply": "2024-12-15T14:43:31.098818Z",
     "shell.execute_reply.started": "2024-12-15T14:43:31.094064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "from miditok import REMI, TokenizerConfig\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from miditok.utils import split_files_for_training\n",
    "from miditok.data_augmentation import augment_dataset\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:43:31.102435Z",
     "iopub.status.busy": "2024-12-15T14:43:31.102066Z",
     "iopub.status.idle": "2024-12-15T14:43:31.217994Z",
     "shell.execute_reply": "2024-12-15T14:43:31.217100Z",
     "shell.execute_reply.started": "2024-12-15T14:43:31.102399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.mixed_precision as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.844605Z",
     "iopub.status.idle": "2024-12-13T15:58:01.845239Z",
     "shell.execute_reply": "2024-12-13T15:58:01.844935Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.844904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths to the files of the dataset\n",
    "\n",
    "midi_paths = list(Path(dataset_path).resolve().glob(\"**/*.mid\")) + list(Path(dataset_path).resolve().glob(\"**/*.midi\"))\n",
    "\n",
    "midis_dir = \"midis\"\n",
    "os.makedirs(midis_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for i, midi_path in enumerate(midi_paths):\n",
    "  new_midi_path = os.path.join(midis_dir, f\"{i}.midi\")\n",
    "  shutil.move(str(midi_path), new_midi_path)\n",
    "\n",
    "\n",
    "midis = list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.mid\")) + list(Path(\"/kaggle/working/midis\").resolve().glob(\"**/*.midi\"))\n",
    "\n",
    "def sample():\n",
    "  return str(random.choice(midis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.848084Z",
     "iopub.status.idle": "2024-12-13T15:58:01.848536Z",
     "shell.execute_reply": "2024-12-13T15:58:01.848337Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.848312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BEAT_RES = {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "\n",
    "TOKENIZER_PARAMS = {\n",
    "\n",
    "    \"pitch_range\": (21, 109),\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"num_velocities\": 24,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\"],\n",
    "    \"use_chords\": True,\n",
    "    \"use_rests\": True,\n",
    "    \"use_tempos\": True,\n",
    "    \"num_tempos\": 32,\n",
    "    \"tempo_range\": (50, 200),  # (min_tempo, max_tempo),\n",
    "}\n",
    "\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.849974Z",
     "iopub.status.idle": "2024-12-13T15:58:01.850680Z",
     "shell.execute_reply": "2024-12-13T15:58:01.850321Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.850288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.train(vocab_size=30000, files_paths=midis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.851961Z",
     "iopub.status.idle": "2024-12-13T15:58:01.852523Z",
     "shell.execute_reply": "2024-12-13T15:58:01.852275Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.852251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed = [Path(f\"{s}\") for s in midis]\n",
    "print(len(processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.855279Z",
     "iopub.status.idle": "2024-12-13T15:58:01.855744Z",
     "shell.execute_reply": "2024-12-13T15:58:01.855520Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.855496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_perc = 0.3\n",
    "\n",
    "total_num_files = len(processed)\n",
    "num_files_valid = round(total_num_files * valid_perc)\n",
    "shuffle(processed)\n",
    "midi_paths_valid = processed[:num_files_valid]\n",
    "midi_paths_train = processed[num_files_valid:]\n",
    "\n",
    "# Chunk MIDIs and perform data augmentation on each subset independently\n",
    "\n",
    "for files_paths, subset_name in (\n",
    "    (midi_paths_train, \"train\"),\n",
    "    (midi_paths_valid, \"valid\"),\n",
    "):\n",
    "    print(files_paths[0])\n",
    "\n",
    "    # Split the MIDIs into chunks of sizes approximately about 1024 tokens\n",
    "\n",
    "    subset_chunks_dir = Path(f\"Maestro_{subset_name}\")\n",
    "\n",
    "    split_files_for_training(\n",
    "        files_paths=files_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=subset_chunks_dir,\n",
    "        max_seq_len=1024,\n",
    "        num_overlap_bars=2,\n",
    "    )\n",
    "\n",
    "    # Perform data augmentation\n",
    "\n",
    "    augment_dataset(\n",
    "        subset_chunks_dir,\n",
    "        pitch_offsets=[-12, 12],\n",
    "        velocity_offsets=[-4, 4],\n",
    "        duration_offsets=[-0.5, 0.5],\n",
    "    )\n",
    "\n",
    "midi_paths_train = list(Path(\"Maestro_train\").glob(\"**/*.mid\")) + list(Path(\"Maestro_train\").glob(\"**/*.midi\"))\n",
    "midi_paths_valid = list(Path(\"Maestro_valid\").glob(\"**/*.mid\")) + list(Path(\"Maestro_valid\").glob(\"**/*.midi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.857138Z",
     "iopub.status.idle": "2024-12-13T15:58:01.857579Z",
     "shell.execute_reply": "2024-12-13T15:58:01.857380Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.857357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def midi_valid(midi) -> bool:\n",
    "\n",
    "    if any(ts.numerator != 4 for ts in midi.time_signature_changes):\n",
    "\n",
    "        return False  # time signature different from 4/*, 4 beats per bar\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(\"tokenized\"):\n",
    "\n",
    "  shutil.rmtree(\"tokenized\")\n",
    "\n",
    "\n",
    "for dir in (\"train\", \"valid\"):\n",
    "    tokenizer.tokenize_dataset(        \n",
    "    \n",
    "        Path(f\"/kaggle/working/Maestro_{dir}\"),\n",
    "        Path(f\"/kaggle/working/tokenized_{dir}\"),\n",
    "        midi_valid,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.858557Z",
     "iopub.status.idle": "2024-12-13T15:58:01.858984Z",
     "shell.execute_reply": "2024-12-13T15:58:01.858787Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.858764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_json(path: str) -> dict:\n",
    "\n",
    "  with open(path, \"r\") as f:\n",
    "\n",
    "    return json.load(f)\n",
    "\n",
    "def read_json_files(json_file_paths):\n",
    "    \"\"\"Reads a list of JSON files and returns a list of objects.\n",
    "    Args:\n",
    "        json_file_paths: A list of file paths to JSON files.\n",
    "    Returns:\n",
    "        A list of objects, where each object represents the data from a JSON file.\n",
    "        Returns an empty list if any error occurs during file processing.\n",
    "    \"\"\"\n",
    "\n",
    "    objects = []\n",
    "\n",
    "    for file_path in tqdm(json_file_paths):\n",
    "\n",
    "        try:\n",
    "\n",
    "            objects.append(read_json(file_path))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            print(f\"Error: File not found - {file_path}\")\n",
    "\n",
    "            return [] # Return empty list on error\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "\n",
    "            print(f\"Error decoding JSON in file: {file_path}\")\n",
    "\n",
    "            return [] # Return empty list on error\n",
    "\n",
    "    return objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.860319Z",
     "iopub.status.idle": "2024-12-13T15:58:01.860751Z",
     "shell.execute_reply": "2024-12-13T15:58:01.860554Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.860531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = list(Path(\"tokenized_train\").resolve().glob(\"**/*.json\"))\n",
    "data_objects_train = read_json_files(tokenized_train)\n",
    "\n",
    "tokenized_valid = list(Path(\"tokenized_valid\").resolve().glob(\"**/*.json\"))\n",
    "data_objects_valid = read_json_files(tokenized_valid)\n",
    "\n",
    "if data_objects_train:\n",
    "    print(f\"\\nSuccessfully read {len(data_objects_train)} training JSON files.\")\n",
    "else:\n",
    "    print(\"Error reading JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.863115Z",
     "iopub.status.idle": "2024-12-13T15:58:01.863721Z",
     "shell.execute_reply": "2024-12-13T15:58:01.863437Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.863405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_train = [np.array(song[\"ids\"][0]) for song in data_objects_train]\n",
    "encoded_valid = [np.array(song[\"ids\"][0]) for song in data_objects_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.865159Z",
     "iopub.status.idle": "2024-12-13T15:58:01.865752Z",
     "shell.execute_reply": "2024-12-13T15:58:01.865471Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.865440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_ids_train = np.concatenate(encoded_train)\n",
    "all_ids_valid = np.concatenate(encoded_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or skip all the data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:43:31.219190Z",
     "iopub.status.busy": "2024-12-15T14:43:31.218912Z",
     "iopub.status.idle": "2024-12-15T14:44:04.966255Z",
     "shell.execute_reply": "2024-12-15T14:44:04.965190Z",
     "shell.execute_reply.started": "2024-12-15T14:43:31.219165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown 1SDRkoWwyuSl4udoCHdcitjLLm9d0kfxS # tokenizer_maestro0612.json\n",
    "!gdown 1IQToXD9s8g4L-AlK-MY4qvGoLZ-p7bMw # ids_train\n",
    "!gdown 1DWjViUKpW07LfbGimlhhhGdK7oQaJpj- # ids_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:44:04.968605Z",
     "iopub.status.busy": "2024-12-15T14:44:04.967847Z",
     "iopub.status.idle": "2024-12-15T14:44:19.768222Z",
     "shell.execute_reply": "2024-12-15T14:44:19.767471Z",
     "shell.execute_reply.started": "2024-12-15T14:44:04.968560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = REMI(params=\"tokenizer_maestro0612.json\")\n",
    "all_ids_train = np.loadtxt(\"ids_train\").astype(dtype=np.int32)\n",
    "all_ids_valid = np.loadtxt(\"ids_valid\").astype(dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended: limit arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:44:19.770058Z",
     "iopub.status.busy": "2024-12-15T14:44:19.769433Z",
     "iopub.status.idle": "2024-12-15T14:44:19.775516Z",
     "shell.execute_reply": "2024-12-15T14:44:19.774629Z",
     "shell.execute_reply.started": "2024-12-15T14:44:19.770016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "perc = 0.3\n",
    "all_ids_train = all_ids_train[:int(perc * len(all_ids_train))]\n",
    "all_ids_valid = all_ids_valid[:int(perc * len(all_ids_valid))]\n",
    "print(f\"Loaded {len(all_ids_train)} training ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:44:19.776811Z",
     "iopub.status.busy": "2024-12-15T14:44:19.776533Z",
     "iopub.status.idle": "2024-12-15T14:44:20.063434Z",
     "shell.execute_reply": "2024-12-15T14:44:20.062545Z",
     "shell.execute_reply.started": "2024-12-15T14:44:19.776785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ids_dataset_train = tf.data.Dataset.from_tensor_slices(all_ids_train)\n",
    "ids_dataset_valid = tf.data.Dataset.from_tensor_slices(all_ids_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:44:20.065176Z",
     "iopub.status.busy": "2024-12-15T14:44:20.064791Z",
     "iopub.status.idle": "2024-12-15T14:44:20.626510Z",
     "shell.execute_reply": "2024-12-15T14:44:20.625559Z",
     "shell.execute_reply.started": "2024-12-15T14:44:20.065134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_length = 512\n",
    "vocab_size = len(tokenizer)\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def normalize_and_split(sequence):\n",
    "    # Convert to float32\n",
    "    input_seq = tf.cast(sequence, tf.float32)\n",
    "    normalized_seq = (input_seq - vocab_size / 2) / (vocab_size / 2)\n",
    "    target = tf.ones_like(normalized_seq)  # Create target tensor with all 1s\n",
    "    return normalized_seq, target\n",
    "\n",
    "train_ds = (\n",
    "    ids_dataset_train\n",
    "    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n",
    "    .map(normalize_and_split)\n",
    "    .map(lambda x, y: (tf.expand_dims(x, -1), y))  # Add channel dimension: (seq_length, 1)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = (\n",
    "    ids_dataset_valid\n",
    "    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n",
    "    .map(normalize_and_split)\n",
    "    .map(lambda x, y: (tf.expand_dims(x, -1), y))  # Add channel dimension: (seq_length, 1)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "for real_seqs, targets in train_ds.take(1):\n",
    "    print(f\"Input Shape: {real_seqs.shape}, Input Type: {real_seqs.dtype}\")\n",
    "    print(f\"Target Shape: {targets.shape}, Target Type: {targets.dtype}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy array version (not updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-13T15:58:01.879100Z",
     "iopub.status.idle": "2024-12-13T15:58:01.879692Z",
     "shell.execute_reply": "2024-12-13T15:58:01.879414Z",
     "shell.execute_reply.started": "2024-12-13T15:58:01.879383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_length = 512\n",
    "vocab_size = len(tokenizer)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def normalize(sequence, vocab_size):\n",
    "    return (sequence - vocab_size / 2) / (vocab_size / 2)\n",
    "\n",
    "def prepare_numpy_dataset(array, seq_length, vocab_size, batch_size, buffer_size):\n",
    "    # 1. Trim the array to be divisible by `seq_length`\n",
    "    num_sequences = len(array) // seq_length\n",
    "    array = array[:num_sequences * seq_length]\n",
    "    \n",
    "    # 2. Reshape into sequences\n",
    "    sequences = array.reshape((-1, seq_length))\n",
    "    print(sequences.shape)\n",
    "    \n",
    "    # 3. Normalize sequences\n",
    "    sequences = normalize(sequences, vocab_size)\n",
    "    \n",
    "    # 5. Shuffle the sequences\n",
    "    np.random.shuffle(sequences)\n",
    "    \n",
    "    # 6. Create batches\n",
    "    num_batches = len(sequences) // batch_size\n",
    "    sequences = sequences[:num_batches * batch_size]  # Trim to make divisible by batch size\n",
    "    batches = sequences.reshape((num_batches, batch_size, seq_length))\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = prepare_numpy_dataset(all_ids_train, seq_length, vocab_size, BATCH_SIZE, BUFFER_SIZE)\n",
    "valid_ds = prepare_numpy_dataset(all_ids_valid, seq_length, vocab_size, BATCH_SIZE, BUFFER_SIZE)\n",
    "print(\"Train batch shape:\", train_ds[0].shape)  # Expected: (64, 512, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T16:01:58.123849Z",
     "iopub.status.busy": "2024-12-13T16:01:58.123489Z",
     "iopub.status.idle": "2024-12-13T16:01:58.131510Z",
     "shell.execute_reply": "2024-12-13T16:01:58.130576Z",
     "shell.execute_reply.started": "2024-12-13T16:01:58.123817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generator(latent_dim, seq_shape): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(latent_dim, 1), return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
    "    model.add(Reshape(seq_shape))\n",
    "    return model\n",
    "\n",
    "def discriminator(seq_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=seq_shape, return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:44:20.629894Z",
     "iopub.status.busy": "2024-12-15T14:44:20.629623Z",
     "iopub.status.idle": "2024-12-15T14:44:20.636531Z",
     "shell.execute_reply": "2024-12-15T14:44:20.635639Z",
     "shell.execute_reply.started": "2024-12-15T14:44:20.629869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generator(latent_dim, seq_shape): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(latent_dim, 1), return_sequences=True))  # Reduced units\n",
    "    model.add(Bidirectional(LSTM(128)))  # Reduced units\n",
    "    model.add(Dense(64))  # Reduced units\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(128))  # Reduced units\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n",
    "    model.add(Reshape(seq_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator(seq_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=seq_shape, return_sequences=True))  # Maintain timestep output\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True)))         # Maintain timestep output\n",
    "    model.add(Dense(1, activation='sigmoid'))         # Predict for each timestep\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:45:01.796363Z",
     "iopub.status.busy": "2024-12-15T14:45:01.795787Z",
     "iopub.status.idle": "2024-12-15T14:45:02.565007Z",
     "shell.execute_reply": "2024-12-15T14:45:02.564096Z",
     "shell.execute_reply.started": "2024-12-15T14:45:01.796308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "  def __init__(self, vocab_size, seq_length, latent_dim = 1000):\n",
    "    self.vocab_size = vocab_size\n",
    "    self.seq_length = seq_length\n",
    "    self.seq_shape = (self.seq_length, 1)\n",
    "    self.latent_dim = latent_dim\n",
    "    self.disc_loss = []\n",
    "    self.gen_loss = []\n",
    "\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    self.discriminator = self.build_discriminator()\n",
    "    self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Build the generator\n",
    "    self.generator = self.build_generator()\n",
    "\n",
    "    # The generator takes noise as input and generates note sequences\n",
    "    z = Input(shape=(self.latent_dim, 1))\n",
    "    generated_seq = self.generator(z)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    self.discriminator.trainable = False\n",
    "\n",
    "    # The discriminator takes generated images as input and determines validity\n",
    "    validity = self.discriminator(generated_seq)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator)\n",
    "    # Trains the generator to fool the discriminator\n",
    "    self.combined = Model(z, validity)\n",
    "    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "  def build_discriminator(self):\n",
    "    model = discriminator(self.seq_shape)\n",
    "\n",
    "    seq = Input(shape=self.seq_shape)\n",
    "    validity = model(seq)\n",
    "\n",
    "    return Model(seq, validity)\n",
    "\n",
    "  def build_generator(self):\n",
    "    model = generator(self.latent_dim, self.seq_shape)\n",
    "\n",
    "    noise = Input(shape=(self.latent_dim, 1))\n",
    "    seq = model(noise)\n",
    "\n",
    "    return Model(noise, seq)\n",
    "\n",
    "  def train(self, epochs, batch_size, train_dataset, valid_dataset, sample_interval=50):\n",
    "    print(\"\\nStarting Training\\n\")\n",
    "\n",
    "    # Training the model\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch + 1,))\n",
    "        \n",
    "        with tqdm(enumerate(train_dataset), total=len(train_dataset)) as pbar:\n",
    "            for step, (real_seqs, targets) in pbar:\n",
    "                \n",
    "                # Random noise for generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n",
    "\n",
    "                # Generate a batch of new note sequences\n",
    "                gen_seqs = self.generator.predict(noise, verbose=0)  # Shape: (batch_size, seq_length, 1)\n",
    "                \n",
    "                # Create targets for fake sequences\n",
    "                fake_targets = tf.zeros_like(targets)  # Match target shape\n",
    "                \n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(real_seqs, targets)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake_targets)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # Train the generator\n",
    "                # The generator is trained to produce sequences that the discriminator classifies as \"real\"\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n",
    "                g_loss = self.combined.train_on_batch(noise, targets)  # Use real targets here\n",
    "                \n",
    "                # Update tqdm description every step\n",
    "                pbar.set_description(\n",
    "                    f\"Step {step} | \" + \n",
    "                    f\"D Loss: {d_loss[0]:.4f}, \" +\n",
    "                    f\"D Accuracy: {100 * d_loss[1]:.2f}%, \" +\n",
    "                    f\"G Loss: {g_loss[0]:.4f}\"\n",
    "                )\n",
    "                \n",
    "        # Print progress and save losses every few epochs\n",
    "        if epoch % sample_interval == 0:\n",
    "            print(\n",
    "                f\"{epoch + 1} / {epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] \"\n",
    "                f\"[G loss: {g_loss[0]:.4f}]\"\n",
    "            )\n",
    "            self.disc_loss.append(d_loss[0])\n",
    "            self.gen_loss.append(g_loss)\n",
    "    \n",
    "    print(\"\\nTraining Complete.\\n\")\n",
    "        \n",
    "  def save(self):\n",
    "    # create Model directory if there isn't exist\n",
    "    if not os.path.exists('Model/'):\n",
    "      os.makedirs('Model/')\n",
    "\n",
    "    # save discriminator and generator trained model\n",
    "    self.discriminator.save('Model/discriminator.h5')\n",
    "    self.generator.save('Model/generator.h5')\n",
    "    print(\"The trained C-RNN-GAN model (generator and discriminator) have been saved in the Model folder.\")\n",
    "\n",
    "\n",
    "  def generate(self):\n",
    "    \"\"\" Use random noise to generate music\"\"\"\n",
    "    \n",
    "    # random noise for network input\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, self.latent_dim, 1))\n",
    "    predictions = self.generator.predict(noise)\n",
    "\n",
    "    # transfer sequence numbers to notes\n",
    "    boundary = int(self.vocab_size / 2)\n",
    "    pred_nums = [x * boundary + boundary for x in predictions[0]]\n",
    "    return pred_nums\n",
    "\n",
    "\n",
    "  def plot_loss(self):\n",
    "    \"\"\" Plot and save discriminator and generator loss functions per epoch diagram\"\"\"\n",
    "    plt.plot(self.disc_loss, c='red')\n",
    "    plt.plot(self.gen_loss, c='blue')\n",
    "    plt.title(\"GAN Loss per Epoch\")\n",
    "    plt.legend(['Discriminator', 'Generator'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    plt.savefig('Result/GAN_Loss_per_Epoch_final.png', transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "model = GAN(vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:45:56.669349Z",
     "iopub.status.busy": "2024-12-15T14:45:56.668879Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "model.train(EPOCHS, BATCH_SIZE, train_ds, valid_ds, sample_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T14:45:38.461554Z",
     "iopub.status.busy": "2024-12-15T14:45:38.460865Z",
     "iopub.status.idle": "2024-12-15T14:45:38.792808Z",
     "shell.execute_reply": "2024-12-15T14:45:38.791888Z",
     "shell.execute_reply.started": "2024-12-15T14:45:38.461514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_ids = np.concatenate(model.generate()).astype(np.int32)\n",
    "print(generated_ids)\n",
    "decoded = tokenizer.decode([generated_ids])\n",
    "decoded.dump_midi(\"generated.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
