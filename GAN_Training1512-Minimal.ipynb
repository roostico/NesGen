{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!pip install pretty_midi\n!pip install miditok\n!pip install midi-clip\n\n!wget https://raw.githubusercontent.com/roostico/NesGen/refs/heads/main/utility.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:42:36.455309Z","iopub.execute_input":"2024-12-15T14:42:36.455669Z","iopub.status.idle":"2024-12-15T14:43:16.462283Z","shell.execute_reply.started":"2024-12-15T14:42:36.455638Z","shell.execute_reply":"2024-12-15T14:43:16.461441Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport pretty_midi\nimport numpy as np\nfrom miditok import REMI, TokenizerConfig\nimport json\nimport tensorflow as tf\nfrom miditok.utils import split_files_for_training\nfrom miditok.data_augmentation import augment_dataset\nimport random\nfrom random import shuffle\n\nimport sys\nimport pickle\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Dropout, LSTM, Bidirectional\nfrom tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:43:31.093491Z","iopub.execute_input":"2024-12-15T14:43:31.094104Z","iopub.status.idle":"2024-12-15T14:43:31.099839Z","shell.execute_reply.started":"2024-12-15T14:43:31.094064Z","shell.execute_reply":"2024-12-15T14:43:31.098818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.mixed_precision as mixed_precision\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n\nimport tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:43:31.102066Z","iopub.execute_input":"2024-12-15T14:43:31.102435Z","iopub.status.idle":"2024-12-15T14:43:31.217994Z","shell.execute_reply.started":"2024-12-15T14:43:31.102399Z","shell.execute_reply":"2024-12-15T14:43:31.2171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown 1SDRkoWwyuSl4udoCHdcitjLLm9d0kfxS # tokenizer_maestro0612.json\n!gdown 1IQToXD9s8g4L-AlK-MY4qvGoLZ-p7bMw # ids_train\n!gdown 1DWjViUKpW07LfbGimlhhhGdK7oQaJpj- # ids_valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:43:31.218912Z","iopub.execute_input":"2024-12-15T14:43:31.21919Z","iopub.status.idle":"2024-12-15T14:44:04.966255Z","shell.execute_reply.started":"2024-12-15T14:43:31.219165Z","shell.execute_reply":"2024-12-15T14:44:04.96519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = REMI(params=\"tokenizer_maestro0612.json\")\nall_ids_train = np.loadtxt(\"ids_train\").astype(dtype=np.int32)\nall_ids_valid = np.loadtxt(\"ids_valid\").astype(dtype=np.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:44:04.967847Z","iopub.execute_input":"2024-12-15T14:44:04.968605Z","iopub.status.idle":"2024-12-15T14:44:19.768222Z","shell.execute_reply.started":"2024-12-15T14:44:04.96856Z","shell.execute_reply":"2024-12-15T14:44:19.767471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tensorflow datasets","metadata":{}},{"cell_type":"markdown","source":"### Recommended: limit arrays","metadata":{}},{"cell_type":"code","source":"perc = 0.3\nall_ids_train = all_ids_train[:int(perc * len(all_ids_train))]\nall_ids_valid = all_ids_valid[:int(perc * len(all_ids_valid))]\nprint(f\"Loaded {len(all_ids_train)} training ids\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:44:19.769433Z","iopub.execute_input":"2024-12-15T14:44:19.770058Z","iopub.status.idle":"2024-12-15T14:44:19.775516Z","shell.execute_reply.started":"2024-12-15T14:44:19.770016Z","shell.execute_reply":"2024-12-15T14:44:19.774629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids_dataset_train = tf.data.Dataset.from_tensor_slices(all_ids_train)\nids_dataset_valid = tf.data.Dataset.from_tensor_slices(all_ids_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:44:19.776533Z","iopub.execute_input":"2024-12-15T14:44:19.776811Z","iopub.status.idle":"2024-12-15T14:44:20.063434Z","shell.execute_reply.started":"2024-12-15T14:44:19.776785Z","shell.execute_reply":"2024-12-15T14:44:20.062545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq_length = 512\nvocab_size = len(tokenizer)\nBATCH_SIZE = 128\nBUFFER_SIZE = 10000\n\ndef normalize_and_split(sequence):\n    # Convert to float32\n    input_seq = tf.cast(sequence, tf.float32)\n    normalized_seq = (input_seq - vocab_size / 2) / (vocab_size / 2)\n    target = tf.ones_like(normalized_seq)  # Create target tensor with all 1s\n    return normalized_seq, target\n\ntrain_ds = (\n    ids_dataset_train\n    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n    .map(normalize_and_split)\n    .map(lambda x, y: (tf.expand_dims(x, -1), y))  # Add channel dimension: (seq_length, 1)\n    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\nvalid_ds = (\n    ids_dataset_valid\n    .batch(seq_length, drop_remainder=True)  # Create sequences of shape (seq_length,)\n    .map(normalize_and_split)\n    .map(lambda x, y: (tf.expand_dims(x, -1), y))  # Add channel dimension: (seq_length, 1)\n    .batch(BATCH_SIZE, drop_remainder=True)  # Batch for training: (batch_size, seq_length, 1)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\nfor real_seqs, targets in train_ds.take(1):\n    print(f\"Input Shape: {real_seqs.shape}, Input Type: {real_seqs.dtype}\")\n    print(f\"Target Shape: {targets.shape}, Target Type: {targets.dtype}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:44:20.064791Z","iopub.execute_input":"2024-12-15T14:44:20.065176Z","iopub.status.idle":"2024-12-15T14:44:20.62651Z","shell.execute_reply.started":"2024-12-15T14:44:20.065134Z","shell.execute_reply":"2024-12-15T14:44:20.625559Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The model","metadata":{}},{"cell_type":"code","source":"def generator(latent_dim, seq_shape): \n    model = Sequential()\n    model.add(Input(shape=(latent_dim, 1)))\n    model.add(LSTM(128, input_shape=(latent_dim, 1), return_sequences=True))  # Reduced units\n    model.add(Bidirectional(LSTM(128)))  # Reduced units\n    model.add(Dense(64))  # Reduced units\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(Dense(128))  # Reduced units\n    model.add(LeakyReLU(negative_slope=0.2))\n    model.add(Dense(np.prod(seq_shape), activation='tanh'))\n    model.add(Reshape(seq_shape))\n    return model\n\n\ndef discriminator(seq_shape):\n    model = Sequential()\n    model.add(Input(shape=seq_shape))\n    model.add(LSTM(256, input_shape=seq_shape, return_sequences=True))  # Maintain timestep output\n    model.add(Bidirectional(LSTM(256, return_sequences=True)))         # Maintain timestep output\n    model.add(Dense(1, activation='sigmoid'))         # Predict for each timestep\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:20:23.731909Z","iopub.execute_input":"2024-12-15T15:20:23.732626Z","iopub.status.idle":"2024-12-15T15:20:23.739156Z","shell.execute_reply.started":"2024-12-15T15:20:23.732586Z","shell.execute_reply":"2024-12-15T15:20:23.738245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GAN():\n  def __init__(self, vocab_size, seq_length, latent_dim = 1000):\n    self.vocab_size = vocab_size\n    self.seq_length = seq_length\n    self.seq_shape = (self.seq_length, 1)\n    self.latent_dim = latent_dim\n    self.disc_loss = []\n    self.gen_loss = []\n\n    # Build and compile the discriminator\n    self.discriminator = discriminator(self.seq_shape)\n    self.discriminator.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n\n    # Build the generator\n    self.generator = generator(self.latent_dim, self.seq_shape)\n\n    # The generator takes noise as input and generates note sequences\n    z = Input(shape=(self.latent_dim, 1))\n    generated_seq = self.generator(z)\n\n    # For the combined model we will only train the generator\n    self.discriminator.trainable = False\n\n    # The discriminator takes generated images as input and determines validity\n    validity = self.discriminator(generated_seq)\n\n    # The combined model  (stacked generator and discriminator)\n    # Trains the generator to fool the discriminator\n    self.combined = Model(z, validity)\n    self.combined.compile(loss='binary_crossentropy', optimizer=\"sgd\")\n\n  def train(self, epochs, batch_size, train_dataset, valid_dataset, sample_interval=50):\n    print(\"\\nStarting Training\\n\")\n\n    # Training the model\n    for epoch in range(epochs):\n        print(\"\\nStart of epoch %d\" % (epoch + 1,))\n        \n        with tqdm(enumerate(train_dataset), total=len(train_dataset)) as pbar:\n            for step, (real_seqs, targets) in pbar:\n                \n                # Random noise for generator input\n                noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n\n                # Generate a batch of new note sequences\n                gen_seqs = self.generator.predict(noise, verbose=0)  # Shape: (batch_size, seq_length, 1)\n                \n                # Create targets for fake sequences\n                fake_targets = tf.zeros_like(targets)  # Match target shape\n                \n                # Train the discriminator\n                d_loss_real = self.discriminator.train_on_batch(real_seqs, targets)\n                d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake_targets)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n                \n                # Train the generator\n                # The generator is trained to produce sequences that the discriminator classifies as \"real\"\n                noise = np.random.normal(0, 1, (batch_size, self.latent_dim, 1))\n                g_loss = self.combined.train_on_batch(noise, targets)  # Use real targets here\n                \n                # Update tqdm description every step\n                pbar.set_description(\n                    f\"Step {step} | \" + \n                    f\"D Loss: {d_loss[0]:.4f}, \" +\n                    f\"D Accuracy: {100 * d_loss[1]:.2f}%, \" +\n                    f\"G Loss: {g_loss[0]:.4f}\"\n                )\n                \n        # Print progress and save losses every few epochs\n        if epoch % sample_interval == 0:\n            print(\n                f\"{epoch + 1} / {epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] \"\n                f\"[G loss: {g_loss[0]:.4f}]\"\n            )\n            self.disc_loss.append(d_loss[0])\n            self.gen_loss.append(g_loss)\n    \n    print(\"\\nTraining Complete.\\n\")\n        \n  def save(self):\n    # create Model directory if there isn't exist\n    if not os.path.exists('Model/'):\n      os.makedirs('Model/')\n\n    # save discriminator and generator trained model\n    self.discriminator.save('Model/discriminator.h5')\n    self.generator.save('Model/generator.h5')\n    print(\"The trained C-RNN-GAN model (generator and discriminator) have been saved in the Model folder.\")\n\n\n  def generate(self):\n    \"\"\" Use random noise to generate music\"\"\"\n    \n    # random noise for network input\n    noise = np.random.normal(0, 1, (BATCH_SIZE, self.latent_dim, 1))\n    predictions = self.generator.predict(noise)\n\n    # transfer sequence numbers to notes\n    boundary = int(self.vocab_size / 2)\n    pred_nums = [x * boundary + boundary for x in predictions[0]]\n    return pred_nums\n\n\n  def plot_loss(self):\n    \"\"\" Plot and save discriminator and generator loss functions per epoch diagram\"\"\"\n    plt.plot(self.disc_loss, c='red')\n    plt.plot(self.gen_loss, c='blue')\n    plt.title(\"GAN Loss per Epoch\")\n    plt.legend(['Discriminator', 'Generator'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n    plt.savefig('Result/GAN_Loss_per_Epoch_final.png', transparent=True)\n    plt.close()\n\nmodel = GAN(vocab_size, seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:20:38.787637Z","iopub.execute_input":"2024-12-15T15:20:38.788327Z","iopub.status.idle":"2024-12-15T15:20:39.020707Z","shell.execute_reply.started":"2024-12-15T15:20:38.788293Z","shell.execute_reply":"2024-12-15T15:20:39.019761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 1\n\n\nmodel.train(EPOCHS, BATCH_SIZE, train_ds, valid_ds, sample_interval=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:31:33.458272Z","iopub.execute_input":"2024-12-15T15:31:33.458647Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generated_ids = np.concatenate(model.generate()).astype(np.int32)\nprint(generated_ids)\ndecoded = tokenizer.decode([generated_ids])\ndecoded.dump_midi(\"generated.mid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T15:27:59.64897Z","iopub.execute_input":"2024-12-15T15:27:59.649745Z","iopub.status.idle":"2024-12-15T15:27:59.993855Z","shell.execute_reply.started":"2024-12-15T15:27:59.649707Z","shell.execute_reply":"2024-12-15T15:27:59.992922Z"}},"outputs":[],"execution_count":null}]}